{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex 8-8. (1) IMDB 데이터셋 분석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmernSTOQEgdEf6W+0p1Wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivrm/ai/blob/main/Ex_8_8_(1)_IMDB_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ryiIeNs1D8Ma"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJpYu_JoETeq",
        "outputId": "0d2a97d7-d306-46a3-f41a-fa152c62ee6f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<UNUSED> the and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 8-8. IMDB 영화리뷰 감성분석 (1) IMDB 데이터셋 분석\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "db5QwSgrjn__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 본격적으로 IMDb 영화리뷰 감성분석 태스크에 도전해 보겠습니다.<br>\n",
        "IMDb Large Movie Dataset은 50000개의 영어로 작성된 영화 리뷰 텍스트로 구성되어 있으며,<br>\n",
        "긍정은 1, 부정은 0의 라벨이 달려 있습니다.<br>\n",
        "2011년 **[Learning Word Vectors for Sentiment Analysis](https://aclanthology.org/P11-1015.pdf)** 논문에서 이 데이터셋을 소개하였습니다.<br>\n",
        "<br>\n",
        "50000개의 리뷰 중 절반인 25000개가 훈련용 데이터, 나머지 25000개를 테스트용 데이터로 사용하도록 지정되어 있습니다.<br>\n",
        "이 데이터셋은 tensorflow Keras 데이터셋 안에 포함되어 있어서 손쉽게 다운로드하여 사용할 수 있습니다.<br>\n",
        "이후 스텝의 IMDb 데이터셋 처리 코드 중 일부는 Tensorflow 튜토리얼에 언급된 데이터 전처리 로직을 참고하였음을 밝힙니다."
      ],
      "metadata": {
        "id": "yk3770jrBnf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "\n",
        "# IMDb 데이터셋 다운로드 \n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
      ],
      "metadata": {
        "id": "Wzg9ZGOTjvMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd29f74-c34a-41d3-fea9-13bc08e9a1b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imdb.load_data() 호출 시 단어사전에 등재할 단어의 개수(num_words)를 10000으로 지정하면,<br>\n",
        "그 개수만큼의 word_to_index 딕셔너리까지 생성된 형태로 데이터셋이 생성됩니다.<br>\n",
        "<br>\n",
        "다운로드한 데이터 실제 예시를 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "I5tx9XenB2h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])  # 1번째 리뷰데이터\n",
        "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
        "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
        "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
      ],
      "metadata": {
        "id": "YypDMSJjjwRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359eb091-f967-45e0-b096-dfe4957bda85"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "라벨:  1\n",
            "1번째 리뷰 문장 길이:  218\n",
            "2번째 리뷰 문장 길이:  189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 데이터가 아니라 이미 숫자로 encode된 텍스트 데이터를 다운로드했음을 확인할 수 있습니다.<br>\n",
        "이미 텍스트가 encode되었으므로 IMDb 데이터셋에는 encode에 사용한 딕셔너리까지 함께 제공합니다."
      ],
      "metadata": {
        "id": "2rjzJlX8B8E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = imdb.get_word_index()\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}\n",
        "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
        "print(word_to_index['the'])  # 1 이 출력됩니다."
      ],
      "metadata": {
        "id": "Tnu8u1MXjwLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913ea531-cd24-41a6-8b01-d2a7288edefa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDb 데이터셋의 텍스트 인코딩을 위한 word_to_index, index_to_word는 아래와 같이 보정되어야 합니다.<br>\n",
        "아래 내용은 Tensorflow 튜토리얼의 가이드를 반영하여 작성하였습니다.<br>\n",
        "word_to_index는 IMDb 텍스트 데이터셋의 단어 출현 빈도 기준으로 내림차수 정렬되어 있습니다."
      ],
      "metadata": {
        "id": "ikOfXiD1Ce7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
        "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
        "\n",
        "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
        "word_to_index[\"<PAD>\"] = 0\n",
        "word_to_index[\"<BOS>\"] = 1\n",
        "word_to_index[\"<UNK>\"] = 2  # unknown\n",
        "word_to_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}\n",
        "\n",
        "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
        "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
        "print(index_to_word[4])     # 'the' 가 출력됩니다."
      ],
      "metadata": {
        "id": "j-GVlqW6jzrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b331ab79-de2a-4dea-b9f8-dc875ce99110"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS>\n",
            "4\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다운로드한 데이터셋이 확인되었습니다. 마지막으로, encode된 텍스트가 정상적으로 decode 되는지 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "VIuY9xTaCkiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_decoded_sentence(x_train[0], index_to_word))\n",
        "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
      ],
      "metadata": {
        "id": "ZkUCfpSfjwDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c829c894-21c5-4192-f139-a0d1a04b5d62"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "라벨:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "decode 한 문장과 라벨을 비교하여 일치하는지 확인해 주세요."
      ],
      "metadata": {
        "id": "aeWToB2OCm4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "LGokUY3OCsjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pad_sequences를 통해 데이터셋 상의 문장의 길이를 통일하는 것을 잊어서는 안됩니다.<br>\n",
        "문장 최대 길이 maxlen의 값 설정도 전체 모델 성능에 영향을 미치게 됩니다.<br>\n",
        "이 길이도 적절한 값을 찾기 위해서는 전체 데이터셋의 분포를 확인해 보는 것이 좋습니다."
      ],
      "metadata": {
        "id": "X_v0GhN6CxO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_data_text = list(x_train) + list(x_test)\n",
        "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
        "num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "num_tokens = np.array(num_tokens)\n",
        "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
        "print('문장길이 평균 : ', np.mean(num_tokens))\n",
        "print('문장길이 최대 : ', np.max(num_tokens))\n",
        "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
        "\n",
        "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "maxlen = int(max_tokens)\n",
        "print('pad_sequences maxlen : ', maxlen)\n",
        "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
      ],
      "metadata": {
        "id": "Z8RA7qNxjv6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacf38e6-cbe5-4d14-e21d-9af48e1ac1a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장길이 평균 :  234.75892\n",
            "문장길이 최대 :  2494\n",
            "문장길이 표준편차 :  172.91149458735703\n",
            "pad_sequences maxlen :  580\n",
            "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 경우에는 maxlen=580이 됩니다.<br>\n",
        "또 한 가지 유의해야 하는 것은,<br>\n",
        "padding 방식을 문장 뒤쪽('post')과 앞쪽('pre') 중 어느 쪽으로 하느냐에 따라,<br>\n",
        "RNN을 이용한 딥러닝 적용 시 성능 차이가 발생한다는 점입니다.<br>\n",
        "두 가지 방식을 한 번씩 다 적용해서 RNN을 학습시켜 보면서 그 결과를 비교해 보시기 바랍니다."
      ],
      "metadata": {
        "id": "bixZTDiWC2l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        value=word_to_index[\"<PAD>\"],\n",
        "                                                        padding='post', # 혹은 'pre'\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                       value=word_to_index[\"<PAD>\"],\n",
        "                                                       padding='post', # 혹은 'pre'\n",
        "                                                       maxlen=maxlen)\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "xHrJHu9sjvtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57210dbb-e23a-42d7-e664-e85d748726f3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 580)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Q9. RNN 활용 시 pad_sequences의 padding 방식은 'post'와 'pre' 중 어느 것이 유리할까요? 그 이유는 무엇일까요?<br>\n",
        "<br>\n",
        "**예시답안**<br>\n",
        "RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 됩니다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적입니다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 됩니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ipJby1ewC7KJ"
      }
    }
  ]
}