{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5UeykLuUBzuxMhx08KwBc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BigjOMJgIObN"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Going Deeper(CV)_BS2\n","\n","---\n","# **14-1. 들어가며**\n","\n","이미지에 사람 얼굴이 다수 포함된 경우에도 빠르게 이를 인식할 수 있는 SSD 모델을 구현, 학습해 보고 이를 이용해 카메라 스티커 앱을 개선해...\n","\n","예상소요시간  |  ⏲  450분\n","\n","---"],"metadata":{"id":"8f9C-FwtwzTV"}},{"cell_type":"markdown","source":["https://d3s0tskafalll9.cloudfront.net/media/core/user/2020/09/03/강상권.png\n","\n","https://d3s0tskafalll9.cloudfront.net/media/core/user/2020/09/03/%EA%B0%95%EC%83%81%EA%B6%8C.png\n","\n","강상권<br>\n","@wer2774<br>\n","스타트업 로민에서 연구원으로 일하고 있습니다.<br>\n","OCR 관련 모델과 상품을 개발 업무를 맡고 있습니다.<br>\n","아직 배울 것이 많아 걱정 반 설렘 반으로 일하고 있습니다."],"metadata":{"id":"erVgLLk8wzam"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"43NwXUI0Bzhm"}},{"cell_type":"markdown","source":["---\n","## **14-1. 들어가며** | 20분\n","\n","---"],"metadata":{"id":"5tQIt01DBhNc"}},{"cell_type":"markdown","source":["### **WIDER FACE 데이터셋**\n","---"],"metadata":{"id":"DpzqJy_H4X22"}},{"cell_type":"markdown","source":["오늘 Face Detection 모델의 학습을 위해 다루게 될 데이터셋은 바로 WIDER FACE 데이터셋입니다. 빠른 인퍼런스 타임을 위해 사용할 YOLO, SSD 같은 single stage model을 학습시키는 것은 흔히 COCO 데이터셋 같은 것이 사용되겠지만, 먼 거리에 흩어져 있는 여러 사람의 얼굴을 빠르게 detect하는 모델을 만들기 위해서는 '보다 넓은 공간에 있는 다수의 사람이 등장하는 이미지 데이터셋'이 보다 적합하겠죠? 아래 그림에서 확인할 수 있듯이 WIDER FACE 데이터셋은 그런 용도로 활용하기에 적절해 보입니다."],"metadata":{"id":"ZO-0Ehde4X9D"}},{"cell_type":"markdown","source":[],"metadata":{"id":"TLcrmR0TIV6g"}},{"cell_type":"markdown","source":["준비물\n","\n","---"],"metadata":{"id":"NXV6DKo_IWAF"}},{"cell_type":"markdown","source":["아래와 같이 작업환경을 준비합시다.\n","\n","$ mkdir -p ~/aiffel/face_detector/assets\n","$ mkdir -p ~/aiffel/face_detector/dataset"],"metadata":{"id":"F9gS_0zvIWCv"}},{"cell_type":"markdown","source":["WIDER FACE 데이터셋 홈페이지의 4개의 zip파일을 사용할 예정입니다. 사용할 WIDER_xxx.zip 파일들은 아래와 같이 구글드라이브에 올라가 있습니다."],"metadata":{"id":"O1CPMNKjIWFO"}},{"cell_type":"markdown","source":["다만 데이터는 이미 준비되어 있으니 다운로드할 필요는 없습니다."],"metadata":{"id":"LaJErEUQIWHv"}},{"cell_type":"markdown","source":["WIDER Face Training Images Google Drive\n","WIDER Face Validation Images Google Drive\n","WIDER Face Testing Images Google Drive\n","Face annotations WIDER FACE 데이터셋 홈페이지"],"metadata":{"id":"KDoqcppYIWKK"}},{"cell_type":"markdown","source":["준비된 데이터를 연결만 해줍시다."],"metadata":{"id":"NGHutkdSIWMf"}},{"cell_type":"markdown","source":["        $ ln -s ~/data/* ~/aiffel/face_detector"],"metadata":{"id":"6npTwqAxIWPI"}},{"cell_type":"markdown","source":["face_detector.zip"],"metadata":{"id":"eD6PVqt9IWRe"}},{"cell_type":"markdown","source":["준비된 데이터에는 9개의 python 모듈 파일이 포함되어 있습니다. 확장자가 py인 파일들인데요. 위 face_detector.zip파일에 들어있는 파일들이니 참고하세요. 이 파일에는 이번 노드의 코드가 모듈 형태로 작성되어 있습니다. python 모듈로 작성된 것과 노드로 작성된 것은 약간 차이가 있지만 핵심 내용은 동일합니다. 학습하는데 참고하세요. 프로젝트에도 도움이 될 수 있습니다."],"metadata":{"id":"O6etqaBYIWT7"}},{"cell_type":"markdown","source":["이제 준비가 끝났으니 시작해 볼까요?"],"metadata":{"id":"ngJJLlYUIWY6"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"cj_dzJB9IWbQ"}},{"cell_type":"markdown","source":["---\n","## **14-2. 데이터셋 전처리(1) 분석** 🔒| 20분\n","\n","---"],"metadata":{"id":"lo8nCOtiIWdz"}},{"cell_type":"markdown","source":["WIDER FACE Bounding Box"],"metadata":{"id":"Ka0wE5FPIWeb"}},{"cell_type":"markdown","source":["오늘 다루게 될 WIDER FACE 데이터셋은 Face detection을 위한 데이터셋이고, 입력데이터는 이미지 파일로, Ground Truth는 Bounding box 정보로 되어 있습니다. 이전 스텝에서 준비한 데이터는 4개의 디렉토리에 들어있습니다."],"metadata":{"id":"82Tc7WeUKAE9"}},{"cell_type":"code","source":["!cd ~/aiffel/face_detector/widerface && ls wider_face_split  WIDER_test  WIDER_train  WIDER_val"],"metadata":{"id":"hsEk-CVqKHrK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```python\n","/bin/bash: line 0: cd: /aiffel/aiffel/face_detector/widerface: No such file or directory\n","```"],"metadata":{"id":"OuYR9wjYKAHt"}},{"cell_type":"markdown","source":["/bin/bash: line 0: cd: /aiffel/aiffel/face_detector/widerface: No such file or directory"],"metadata":{"id":"zGNXIvPQKAM3"}},{"cell_type":"markdown","source":["여기서 WIDER_xxxx 로 되어 있는 3개의 디렉토리에는 입력용 이미지 파일만 들어 있습니다. 좀더 구체적으로 분석해 보아야 할 것은 wider_face_split 디렉토리 내에 있는 wider_face_train_bbx_gt.txt과 wider_face_val_bbx_gt.txt, 이 2개 파일 안에 포함되어 있는 Bounding box 정보입니다."],"metadata":{"id":"atOVdAEqKAPj"}},{"cell_type":"markdown","source":["실제 이 파일들이 어떻게 생겼는지 열어볼까요?"],"metadata":{"id":"kon4-ROoKAR3"}},{"cell_type":"code","source":["!cd ~/aiffel/face_detector/widerface/wider_face_split && head -20 wider_face_train_bbx_gt.txt"],"metadata":{"id":"g2FSG-vtKeyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["위 명령어의 결과는 아래와 같습니다."],"metadata":{"id":"2KZMezNCKAUY"}},{"cell_type":"markdown","source":[],"metadata":{"id":"5HykiqY9KAWw"}},{"cell_type":"markdown","source":["    0--Parade/0_Parade_marchingband_1_849.jpg\n","    1\n","    449 330 122 149 0 0 0 0 0 0 \n","    0--Parade/0_Parade_Parade_0_904.jpg\n","    1\n","    361 98 263 339 0 0 0 0 0 0 \n","    0--Parade/0_Parade_marchingband_1_799.jpg\n","    21\n","    78 221 7 8 2 0 0 0 0 0 \n","    78 238 14 17 2 0 0 0 0 0 \n","    113 212 11 15 2 0 0 0 0 0 \n","    134 260 15 15 2 0 0 0 0 0 \n","    163 250 14 17 2 0 0 0 0 0 \n","    201 218 10 12 2 0 0 0 0 0 \n","    182 266 15 17 2 0 0 0 0 0 \n","    245 279 18 15 2 0 0 0 0 0 \n","    304 265 16 17 2 0 0 0 2 1 \n","    328 295 16 20 2 0 0 0 0 0 \n","    (이하생략)"],"metadata":{"id":"L68Y0DOfKAZI"}},{"cell_type":"markdown","source":["텍스트로 이루어진 이 파일 포맷은 다음과 같은 반복 구조로 이루어져 있음을 쉽게 파악할 수 있습니다."],"metadata":{"id":"wBavoVU4KAbn"}},{"cell_type":"markdown","source":["    # 이미지 파일 경로\n","    0--Parade/0_Parade_marchingband_1_849.jpg\n","    # face bounding box 개수\n","    1\n","    # face bounding box 좌표 등 상세정보\n","    449 330 122 149 0 0 0 0 0 0 "],"metadata":{"id":"_xffUk98KAeJ"}},{"cell_type":"markdown","source":["10개의 숫자로 이루어진 face bounding box 좌표 등의 상세정보는 다음과 같은 의미를 가집니다."],"metadata":{"id":"h9fH5cnfKAgh"}},{"cell_type":"markdown","source":["    x0, y0, w, h, blur, expression, illumination, invalid, occlusion, pose"],"metadata":{"id":"mi0gEoNCKAi8"}},{"cell_type":"markdown","source":["bounding box가 관련해서 가장 중요한 4개의 숫자는 왼쪽의 4개(죄상 꼭짓점의 X 좌표, Y 좌표, 너비, 높이) 입니다."],"metadata":{"id":"qwVUoKIjIWga"}},{"cell_type":"markdown","source":["본격적으로 코드를 다루기 전에 필요한 라이브러리와 전역 변수를 준비합니다."],"metadata":{"id":"1D535HTFK2w0"}},{"cell_type":"code","source":["import os, cv2, time\n","import tensorflow as tf\n","import tqdm\n","import numpy as np\n","import math\n","from itertools import product\n","import matplotlib.pyplot as plt\n","\n","PROJECT_PATH = os.getenv('HOME')+'/aiffel/face_detector'\n","DATA_PATH = os.path.join(PROJECT_PATH, 'widerface')\n","MODEL_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n","TRAIN_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'train_mask.tfrecord')\n","VALID_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'val_mask.tfrecord')\n","CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n","\n","DATASET_LEN = 12880\n","BATCH_SIZE = 32\n","IMAGE_WIDTH = 320\n","IMAGE_HEIGHT = 256\n","IMAGE_LABELS = ['background', 'face']\n","\n","print(tf.__version__)"],"metadata":{"id":"RUBg8A44K5cf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["먼저 bounding box 파일을 분석해 봅시다. 분석에 필요한 코드를 함수 형태로 준비할게요."],"metadata":{"id":"Xp_VKh0sK2uS"}},{"cell_type":"code","source":["def parse_box(data):\n","    x0 = int(data[0])\n","    y0 = int(data[1])\n","    w = int(data[2])\n","    h = int(data[3])\n","    return x0, y0, w, h\n","\n","print('슝=3')"],"metadata":{"id":"RnxT4dPOLCH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_widerface(file):\n","    infos = []\n","    with open(file) as fp:\n","        line = fp.readline()\n","        while line:\n","            n_object = int(fp.readline())\n","            boxes = []\n","            for i in range(n_object):\n","                box = fp.readline().split(' ')\n","                x0, y0, w, h = parse_box(box)\n","                if (w == 0) or (h == 0):\n","                    continue\n","                boxes.append([x0, y0, w, h])\n","            if n_object == 0:\n","                box = fp.readline().split(' ')\n","                x0, y0, w, h = parse_box(box)\n","                boxes.append([x0, y0, w, h])\n","            infos.append((line.strip(), boxes))\n","            line = fp.readline()\n","    return infos\n","\n","print('슝=3')"],"metadata":{"id":"4JALn4Z7RoeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["위 함수는 이미지별 bounding box 정보를 wider_face_train_bbx_gt.txt에서 파싱해서 리스트로 추출하는 것입니다.\n","\n","이제 추출된 정보를 실제 이미지 정보와 결합합니다. bounding box 정보는 [x, y, w, h] 형태로 저장되어 있는데, [x_min, y_min, x_max, y_max] 형태의 꼭짓점 좌표 정보로 변환할 거예요.\n","\n","이렇게 정보를 결합해야 나중에 학습에 사용하기 좋습니다."],"metadata":{"id":"8a3pM4BOK2r4"}},{"cell_type":"code","source":["def process_image(image_file):\n","    image_string = tf.io.read_file(image_file)\n","    try:\n","        image_data = tf.image.decode_jpeg(image_string, channels=3)\n","        return 0, image_string, image_data\n","    except tf.errors.InvalidArgumentError:\n","        return 1, image_string, None\n","\n","print('슝=3')"],"metadata":{"id":"EkPxw9EDRtEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def xywh_to_voc(file_name, boxes, image_data):\n","    shape = image_data.shape\n","    image_info = {}\n","    image_info['filename'] = file_name\n","    image_info['width'] = shape[1]\n","    image_info['height'] = shape[0]\n","    image_info['depth'] = 3\n","\n","    difficult = []\n","    classes = []\n","    xmin, ymin, xmax, ymax = [], [], [], []\n","\n","    for box in boxes:\n","        classes.append(1)\n","        difficult.append(0)\n","        xmin.append(box[0])\n","        ymin.append(box[1])\n","        xmax.append(box[0] + box[2])\n","        ymax.append(box[1] + box[3])\n","    image_info['class'] = classes\n","    image_info['xmin'] = xmin\n","    image_info['ymin'] = ymin\n","    image_info['xmax'] = xmax\n","    image_info['ymax'] = ymax\n","    image_info['difficult'] = difficult\n","\n","    return image_info\n","\n","print('슝=3')"],"metadata":{"id":"by4Q73NTRvBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["잘 결합된 데이터의 형태를 확인해 봅시다. 5개만 출력해 볼 거예요."],"metadata":{"id":"xdozsW3URw9X"}},{"cell_type":"code","source":["file_path = os.path.join(DATA_PATH, 'wider_face_split', 'wider_face_train_bbx_gt.txt')\n","for i, info in enumerate(parse_widerface(file_path)):\n","    print('--------------------')\n","    image_file = os.path.join(DATA_PATH, 'WIDER_train', 'images', info[0])\n","    _, image_string, image_data = process_image(image_file)\n","    boxes = xywh_to_voc(image_file, info[1], image_data)\n","    print(boxes)\n","    if i > 3:\n","        break"],"metadata":{"id":"DXq8V67ARyew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["어떤가요? 이미지별로 boxes 리스트에 담긴 bounding box 정보가 확인되시나요? 이제 이 정보를 활용하여 텐서플로우 데이터셋을 생성해 봅시다."],"metadata":{"id":"iFyZyKuIRz6Y"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"apRVRDHeK2pV"}},{"cell_type":"markdown","source":["---\n","## **14-3. 데이터셋 전처리(2) TFRecord 생성** 🔒| 30분\n","\n","---"],"metadata":{"id":"6RLigxfrK2mw"}},{"cell_type":"markdown","source":["TFRecord 만들기"],"metadata":{"id":"Lm--AYuHK2km"}},{"cell_type":"markdown","source":["오늘 다루게 될 대용량 데이터셋의 처리속도 향상을 위해서, 전처리 작업을 통해 TFRecord 데이터셋으로 변환할 필요가 있습니다. TFRecord란 TensorFlow만의 학습 데이터 저장 포맷으로, 이진(binary) 레코드의 시퀀스를 저장합니다. TFRecord 형태의 학습 데이터를 사용하여 모델 학습을 하면 학습 속도가 개선된다는 장점이 있습니다."],"metadata":{"id":"ed6OLPbGK2iA"}},{"cell_type":"markdown","source":["TFRecord는 여러 개의 tf.train.Example로 이루어져 있고, 한 개의 tf.train.Example은 여러 개의 tf.train.Feature로 이루어져 있습니다."],"metadata":{"id":"zdV4_lZnSBvR"}},{"cell_type":"markdown","source":["데이터의 단위를 이루는 tf.train.Example 인스턴스를 생성하는 메소드는 아래와 같습니다."],"metadata":{"id":"if1ivkO4SByb"}},{"cell_type":"code","source":["def make_example(image_string, image_infos):\n","    for info in image_infos:\n","        filename = info['filename']\n","        width = info['width']\n","        height = info['height']\n","        depth = info['depth']\n","        classes = info['class']\n","        xmin = info['xmin']\n","        ymin = info['ymin']\n","        xmax = info['xmax']\n","        ymax = info['ymax']\n","\n","    if isinstance(image_string, type(tf.constant(0))):\n","        encoded_image = [image_string.numpy()]\n","    else:\n","        encoded_image = [image_string]\n","\n","    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n","    \n","    example = tf.train.Example(features=tf.train.Features(feature={\n","        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n","        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n","        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n","        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n","        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n","        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n","        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n","        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n","        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n","    }))\n","    \n","    return example\n","\n","print('슝=3')"],"metadata":{"id":"Lag2THm5SIDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 전처리를 위해 필요한 함수들이 어느 정도 갖추어졌습니다. 데이터셋의 이미지 파일, 그리고 bounding box 정보를 모아 위의 make_example 메소드를 통해 만든 example을 serialize하여 TFRecord 파일로 생성하게 됩니다."],"metadata":{"id":"yVRhbPbxSB40"}},{"cell_type":"markdown","source":["TFRecord에 대한 자세한 사항은 아래를 참고하세요.\n","\n","TFRecord"],"metadata":{"id":"wtJMS36xSB7f"}},{"cell_type":"code","source":["for split in ['train', 'val']:\n","    if split == 'train':\n","        output_file = TRAIN_TFRECORD_PATH \n","        anno_txt = 'wider_face_train_bbx_gt.txt'\n","        file_path = 'WIDER_train'\n","    else:\n","        output_file = VALID_TFRECORD_PATH\n","        anno_txt = 'wider_face_val_bbx_gt.txt'\n","        file_path = 'WIDER_val'\n","\n","    with tf.io.TFRecordWriter(output_file) as writer:\n","        for info in tqdm.tqdm(parse_widerface(os.path.join(DATA_PATH, 'wider_face_split', anno_txt))):\n","            image_file = os.path.join(DATA_PATH, file_path, 'images', info[0])\n","            error, image_string, image_data = process_image(image_file)\n","            boxes = xywh_to_voc(image_file, info[1], image_data)\n","\n","            if not error:\n","                tf_example = make_example(image_string, [boxes])\n","                writer.write(tf_example.SerializeToString())"],"metadata":{"id":"zxzXWYJzSNNQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["생성된 TFRecord 파일을 확인해 봅시다."],"metadata":{"id":"IE_nsDqiSB96"}},{"cell_type":"code","source":["!ls ~/aiffel/face_detector/dataset"],"metadata":{"id":"N12NuBUhSRGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"au6kOg_OSCAc"}},{"cell_type":"markdown","source":["---\n","## **14-4. 모델 구현(1) Default boxes** 🔒| 30분\n","\n","---"],"metadata":{"id":"rAgWxQSlSCDE"}},{"cell_type":"markdown","source":["SSD의 Default box\n","\n","---"],"metadata":{"id":"AO3luexmSCFc"}},{"cell_type":"markdown","source":["SSD 모델의 가장 중요한 특징 중 하나는 Default box를 필요로 한다는 점입니다. 앞서 이야기한 대로 Default box란, object가 존재할 만한 다양한 크기의 box의 좌표 및 클래스 정보를 일정 개수만큼 미리 고정해 둔 것입니다. 흔히 anchor box, prior box라고 부르지요. SSD의 Default box가 약간 다른 점은 여러 층의 feature map에서 box를 만들어 낸다는 점이에요. 층 수 만큼 box 수도 많아지고, 층마다 box의 크기도 다양하게 되죠. ground truth에 해당하는 bounding box와의 IoU를 계산하여 일정 크기(0.5) 이상 겹치는 default box를 선택하는 방식이 RCNN 계열의 sliding window 방식보다 훨씬 속도가 빠르면서도 그와 유사한 정도의 정확도를 얻을 수 있습니다."],"metadata":{"id":"ZBNm5Vp_K2fT"}},{"cell_type":"markdown","source":[],"metadata":{"id":"uPCZZYJYSY1K"}},{"cell_type":"markdown","source":["이번 프로젝트에서 사용할 default box 정보를 전역 변수로 만들어 둡니다."],"metadata":{"id":"CX7_KffySY3v"}},{"cell_type":"code","source":["BOX_MIN_SIZES = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]\n","BOX_STEPS = [8, 16, 32, 64]\n","\n","print('슝=3')"],"metadata":{"id":"M5gF2w_YSf0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"PyhgVSXASY6P"}},{"cell_type":"markdown","source":["위 그림에서 보는 것처럼, default box를 생성하기 위해서는 먼저 기준이 되는 feature map을 먼저 생성합니다. 그림에서는 8 X 8, 4 X 4의 예가 나오지만, 우리의 프로젝트에서는 아래와 같이 4가지 유형의 feature map을 생성하게 됩니다."],"metadata":{"id":"nogDAWxYSY8h"}},{"cell_type":"code","source":["image_sizes = (IMAGE_HEIGHT, IMAGE_WIDTH)\n","min_sizes = BOX_MIN_SIZES\n","steps= BOX_STEPS\n","\n","feature_maps = [\n","    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n","    for step in steps\n","]\n","feature_maps"],"metadata":{"id":"rgbx0OkrSk0g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 feature map별로 순회를 하면서 default box 를 생성해 보겠습니다."],"metadata":{"id":"HeSaTIfnSY-7"}},{"cell_type":"code","source":["boxes = []\n","for k, f in enumerate(feature_maps):\n","    for i, j in product(range(f[0]), range(f[1])):\n","        for min_size in min_sizes[k]:\n","            s_kx = min_size / image_sizes[1]\n","            s_ky = min_size / image_sizes[0]\n","            cx = (j + 0.5) * steps[k] / image_sizes[1]\n","            cy = (i + 0.5) * steps[k] / image_sizes[0]\n","            boxes += [cx, cy, s_kx, s_ky]\n","\n","len(boxes)"],"metadata":{"id":"YPmRQYr1Sn_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["생성된 boxes에는 default box 정보가 구분없이 나열되어 있으므로 4개씩 재배열 시켜줄게요."],"metadata":{"id":"D-jUsZ_0SZBP"}},{"cell_type":"code","source":["pretty_boxes = np.asarray(boxes).reshape([-1, 4])\n","print(pretty_boxes.shape)\n","print(pretty_boxes)"],"metadata":{"id":"-Zc7QvIUSqgA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모두 4700개의 default box가 만들어 졌네요. feature_maps와 min_sizes로부터 40x32x3 + 20x16x2 + 10x8x2 + 5x4x3 개가 생성되었다는 걸 확인할 수 있습니다."],"metadata":{"id":"Afy5j5m-SZD9"}},{"cell_type":"markdown","source":["지금까지 만들었던, feature map을 만들고 그에 연결된 default box를 생성하는 코드를 사용하기 편리하도록 함수로 정의해 둡시다."],"metadata":{"id":"NGjZielwSZFx"}},{"cell_type":"code","source":["def default_box():\n","    image_sizes = (IMAGE_HEIGHT, IMAGE_WIDTH)\n","    min_sizes = BOX_MIN_SIZES\n","    steps= BOX_STEPS\n","    feature_maps = [\n","        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n","        for step in steps\n","    ]\n","    boxes = []\n","    for k, f in enumerate(feature_maps):\n","        for i, j in product(range(f[0]), range(f[1])):\n","            for min_size in min_sizes[k]:\n","                s_kx = min_size / image_sizes[1]\n","                s_ky = min_size / image_sizes[0]\n","                cx = (j + 0.5) * steps[k] / image_sizes[1]\n","                cy = (i + 0.5) * steps[k] / image_sizes[0]\n","                boxes += [cx, cy, s_kx, s_ky]\n","    boxes = np.asarray(boxes).reshape([-1, 4])\n","    return boxes\n","\n","print('슝=3')"],"metadata":{"id":"cTv56ZVLSumJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"-0CsaFOcSZIo"}},{"cell_type":"markdown","source":["---\n","## **14-5. 모델 구현(2) SSD** 🔒| 30분\n","\n","---"],"metadata":{"id":"9LiUlYEdSZKd"}},{"cell_type":"markdown","source":["SSD model 빌드하기\n","\n","---"],"metadata":{"id":"Jsdu772ESZNz"}},{"cell_type":"markdown","source":["그럼 본격적으로 SSD 모델을 생성해 보겠습니다. 우선은 SSD 모델 내부에서 사용하는 레이어들을 아래와 같이 생성합니다."],"metadata":{"id":"jy39sk7-S70d"}},{"cell_type":"markdown","source":["일반적으로 많이 쓰이는 Convolution 블록, Depthwise Convolution 블록, 그리고 skip connection으로 쓰일 Branch 블록을 준비합니다."],"metadata":{"id":"9qjLa88yS73S"}},{"cell_type":"code","source":["def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1)):\n","    block_id = (tf.keras.backend.get_uid())\n","    if strides == (2, 2):\n","        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n","        x = tf.keras.layers.Conv2D(filters, kernel,\n","                                   padding='valid',\n","                                   use_bias=False,\n","                                   strides=strides,\n","                                   name='conv_%d' % block_id)(x)\n","    else:\n","        x = tf.keras.layers.Conv2D(filters, kernel,\n","                                   padding='same',\n","                                   use_bias=False,\n","                                   strides=strides,\n","                                   name='conv_%d' % block_id)(inputs)\n","    \n","    x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n","    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)\n","\n","print('슝=3')"],"metadata":{"id":"9Sq_VPPkTCYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _depthwise_conv_block(inputs, filters, strides=(1, 1)):\n","    block_id = tf.keras.backend.get_uid()\n","    if strides == (1, 1):\n","        x = inputs\n","    else:\n","        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n","    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n","                                        padding='same' if strides == (1, 1) else 'valid',\n","                                        strides=strides,\n","                                        use_bias=False,\n","                                        name='conv_dw_%d' % block_id)(x)\n","    x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n","    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n","    x = tf.keras.layers.Conv2D(filters, (1, 1),\n","                               padding='same',\n","                               use_bias=False,\n","                               strides=(1, 1),\n","                               name='conv_pw_%d' % block_id)(x)\n","    x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n","    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)\n","\n","print('슝=3')"],"metadata":{"id":"-Yo5e1WGTFAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _branch_block(inputs, filters):\n","    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(inputs)\n","    x = tf.keras.layers.LeakyReLU()(x)\n","    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n","    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(inputs)\n","    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n","    return tf.keras.layers.ReLU()(x)\n","\n","print('슝=3')"],"metadata":{"id":"QTUmuLu9TG4U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["여러 블록을 쌓아 모델을 만든 후, 중간중간 Branch 부분에 헤드(head) 라고 불리는 Convolution 레이어를 붙일 거예요. 하나의 헤드에 Convolution 레이어 두 개가 필요합니다. 하나는 confidence를 예측하기 위해 사용하고 다른 하나는 location을 예측하기 위해 사용해요."],"metadata":{"id":"yu_CFbVPS75u"}},{"cell_type":"markdown","source":["여러 블록을 쌓아 모델을 만든 후, 중간중간 Branch 부분에 헤드(head) 라고 불리는 Convolution 레이어를 붙일 거예요. 하나의 헤드에 Convolution 레이어 두 개가 필요합니다. 하나는 confidence를 예측하기 위해 사용하고 다른 하나는 location을 예측하기 위해 사용해요."],"metadata":{"id":"d9k33-zUS776"}},{"cell_type":"code","source":["def _create_head_block(inputs, filters):\n","    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n","    return x\n","\n","print('슝=3')"],"metadata":{"id":"lGTFe7oBTL5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _compute_heads(inputs, num_class, num_cell):\n","    conf = _create_head_block(inputs, num_cell * num_class)\n","    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n","    loc = _create_head_block(inputs, num_cell * 4)\n","    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n","    return conf, loc\n","\n","print('슝=3')"],"metadata":{"id":"2aYEuPpfTNqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["레이어들이 준비되었습니다. 이제 본격적으로 SSD model을 준비해 보겠습니다."],"metadata":{"id":"NNQ_2IQIS7-Z"}},{"cell_type":"code","source":["def SsdModel():\n","    base_channel = 16\n","    num_cells = [3, 2, 2, 3]\n","    num_class = len(IMAGE_LABELS)\n","    \n","    x = inputs = tf.keras.layers.Input(shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 3], name='input_image')\n","\n","    x = _conv_block(x, base_channel, strides=(2, 2))\n","    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n","    x = _conv_block(x, base_channel * 2, strides=(2, 2))\n","    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n","    x = _conv_block(x, base_channel * 4, strides=(2, 2))\n","    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n","    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n","    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n","    x1 = _branch_block(x, base_channel)\n","\n","    x = _conv_block(x, base_channel * 8, strides=(2, 2))\n","    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n","    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n","    x2 = _branch_block(x, base_channel)\n","\n","    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))\n","    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n","    x3 = _branch_block(x, base_channel)\n","\n","    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))\n","    x4 = _branch_block(x, base_channel)\n","\n","    extra_layers = [x1, x2, x3, x4]\n","\n","    confs = []\n","    locs = []\n","\n","    for layer, num_cell in zip(extra_layers, num_cells):\n","        conf, loc = _compute_heads(layer, num_class, num_cell)\n","        confs.append(conf)\n","        locs.append(loc)\n","\n","    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n","    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n","\n","    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=predictions, name='ssd_model')\n","    return model\n","\n","print('슝=3')"],"metadata":{"id":"0WT_rz9eTRGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["준비한 모델을 생성해 봅시다."],"metadata":{"id":"VJI4rs80S8As"}},{"cell_type":"code","source":["model = SsdModel()\n","print(\"the number of model layers: \", len(model.layers))\n","model.summary()"],"metadata":{"id":"eMwV6EmYTUMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["101층의 SSD 모델이 완성되었습니다!"],"metadata":{"id":"YxGFRy_nS8C3"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"4aZlSAeZK2cn"}},{"cell_type":"markdown","source":["---\n","## **14-6. 모델 학습(1) Augmentation, jaccard 적용** 🔒| 30분\n","\n","---"],"metadata":{"id":"8j7q7T3-K2Zy"}},{"cell_type":"markdown","source":["### **Augmentation**\n","---"],"metadata":{"id":"2T0W6RP8IWix"}},{"cell_type":"markdown","source":["이전 스텝에서 모델까지 구현하였습니다. 그러나 본격적으로 학습을 진행하기 전에 아직 해야할 것이 몇 가지 더 남아있습니다."],"metadata":{"id":"6l9DDXeaTk-M"}},{"cell_type":"markdown","source":["이전 스텝에서 구성한 TFRecord 형태의 데이터셋은 아직 Data augmentation이 적용되지 않았습니다. Object detection에서 사용하는 다양한 augmentation 기법을 적용해 주면 성능이 조금 더 향상될 수도 있습니다."],"metadata":{"id":"F4Cwo8dnTk_W"}},{"cell_type":"markdown","source":["아래는 augmentation을 위해 tf.data.TFRecordDataset.map() 내에서 호출할 메소드들입니다."],"metadata":{"id":"yqf8Bg1ITlBM"}},{"cell_type":"markdown","source":["* _crop\n","* _pad_to_square\n","* _resize\n","* _flip\n","* _distort"],"metadata":{"id":"YeswA1y0TlFi"}},{"cell_type":"markdown","source":["Image Classification에서 사용하던 함수와는 비교가 안 될 정도로 복잡한 함수들입니다. 이미지가 변경되면서 box의 위치나 크기도 함께 변경되어야 하기 때문에 함수가 복잡해질 수 밖에 없어요."],"metadata":{"id":"2OVXr5iPTlHs"}},{"cell_type":"markdown","source":["이미지를 _crop했을 때 box도 함께 잘릴 수가 있어요. 함께 신경써서 잘라줍니다."],"metadata":{"id":"OpthQUEYTlKM"}},{"cell_type":"code","source":["def _crop(img, labels, max_loop=250):\n","    shape = tf.shape(img)\n","\n","    def matrix_iof(a, b):\n","        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n","        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n","\n","        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n","            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n","        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n","        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n","\n","    def crop_loop_body(i, img, labels):\n","        valid_crop = tf.constant(1, tf.int32)\n","\n","        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n","        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n","        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n","        h = w = tf.cast(scale * short_side, tf.int32)\n","        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n","        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n","        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n","        roi = tf.cast(roi, tf.float32)\n","\n","        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n","        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n","                             lambda: valid_crop, lambda: 0)\n","\n","        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n","        mask_a = tf.reduce_all(\n","            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n","            axis=1)\n","        labels_t = tf.boolean_mask(labels, mask_a)\n","        valid_crop = tf.cond(tf.reduce_any(mask_a),\n","                             lambda: valid_crop, lambda: 0)\n","\n","        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n","        h_offset = tf.cast(h_offset, tf.float32)\n","        w_offset = tf.cast(w_offset, tf.float32)\n","        labels_t = tf.stack(\n","            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n","             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n","             labels_t[:, 4]], axis=1)\n","\n","        return tf.cond(valid_crop == 1,\n","                       lambda: (max_loop, img_t, labels_t),\n","                       lambda: (i + 1, img, labels))\n","\n","    _, img, labels = tf.while_loop(\n","        lambda i, img, labels: tf.less(i, max_loop),\n","        crop_loop_body,\n","        [tf.constant(-1), img, labels],\n","        shape_invariants=[tf.TensorShape([]),\n","                          tf.TensorShape([None, None, 3]),\n","                          tf.TensorShape([None, 5])])\n","\n","    return img, labels\n","\n","print('슝=3')"],"metadata":{"id":"3XCyzKZKTzxp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["_resize나 _flip도 box에 영향을 끼칩니다."],"metadata":{"id":"NcbijfkoTlMy"}},{"cell_type":"code","source":["def _resize(img, labels):\n","    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n","    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n","    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n","                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n","    locs = tf.clip_by_value(locs, 0, 1.0)\n","    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n","\n","    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n","\n","    def resize(method):\n","        def _resize():\n","            #　size h,w\n","            return tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH], method=method, antialias=True)\n","        return _resize\n","\n","    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n","                   (tf.equal(resize_case, 1), resize('area')),\n","                   (tf.equal(resize_case, 2), resize('nearest')),\n","                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n","                  default=resize('bilinear'))\n","\n","    return img, labels\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCArtyy6T3gh","executionInfo":{"status":"ok","timestamp":1666251209217,"user_tz":-540,"elapsed":4,"user":{"displayName":"김용식","userId":"14840865344892218183"}},"outputId":"0aa7b4f1-b0fa-4fc3-9d3d-e43fc2d55508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"code","source":["def _flip(img, labels):\n","    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n","\n","    def flip_func():\n","        flip_img = tf.image.flip_left_right(img)\n","        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n","                                1 - labels[:, 0],  labels[:, 3],\n","                                labels[:, 4]], axis=1)\n","\n","        return flip_img, flip_labels\n","\n","    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n","\n","    return img, labels\n","\n","print('슝=3')"],"metadata":{"id":"3sg-N5mpT520"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["_pad_to_square는 이미지가 정사각형이 되도록 이미지 오른쪽이나 아래 방향으로 평균 색상 영역을 추가해주는 작업입니다.\n","\n","다행히도 이미지 왼쪽이나 위쪽으로는 변화가 없기 때문에 box 정보는 변하지 않습니다."],"metadata":{"id":"4UcONdCuTlO8"}},{"cell_type":"code","source":["def _pad_to_square(img):\n","    height = tf.shape(img)[0]\n","    width = tf.shape(img)[1]\n","\n","    def pad_h():\n","        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n","        return tf.concat([img, img_pad_h], axis=0)\n","\n","    def pad_w():\n","        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n","        return tf.concat([img, img_pad_w], axis=1)\n","\n","    img = tf.case([(tf.greater(height, width), pad_w),\n","                   (tf.less(height, width), pad_h)], default=lambda: img)\n","    return img\n","\n","print('슝=3')"],"metadata":{"id":"KQK21wfWT-NN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이미지 색상 값만 바꿔주는 _distort에서도 box 정보를 바꿀 필요는 없겠네요!"],"metadata":{"id":"1VTrW0PdTlRb"}},{"cell_type":"code","source":["def _distort(img):\n","    img = tf.image.random_brightness(img, 0.4)\n","    img = tf.image.random_contrast(img, 0.5, 1.5)\n","    img = tf.image.random_saturation(img, 0.5, 1.5)\n","    img = tf.image.random_hue(img, 0.1)\n","\n","    return img\n","\n","print('슝=3')"],"metadata":{"id":"y2KdCoi9gNBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Default box 적용**\n","---"],"metadata":{"id":"lNce_J9STlUZ"}},{"cell_type":"markdown","source":["SSD 모델의 특이점 중 하나가 default box를 사용한다는 점을 이미 앞에서 설명하였습니다. default box 정보는 데이터셋에 반영되어야 합니다. 아래 메소드들은 default box와 bounding box 사이의 IoU, 다른 말로 자카드 유사도(자카드 지수, jaccard index)를 측정하기 위한 것입니다."],"metadata":{"id":"tq_Ulk8dTlWa"}},{"cell_type":"markdown","source":["자카드 유사도는 두 집합을 통해 유사도를 측정하는 방식 중 하나로, 두 집합의 교집합을 두 집합의 합집합으로 나눕니다. 따라서 자카드 유사도는 0과 1 사이의 값을 가지며, 두 집합의 합집합과 교집합이 서로 비슷하면 1에 근접하다는 뜻입니다."],"metadata":{"id":"v-5-yo3dUC6t"}},{"cell_type":"markdown","source":["자카드 유사도의 공식은 아래와 같습니다."],"metadata":{"id":"d75LQv39UC9L"}},{"cell_type":"markdown","source":["자카드 유사도\n","J(X, Y)=\\frac{|X \\cap Y|}{|X U Y|}=\\frac{|X \\cap Y|}{|X|+|Y|-|X \\cap Y|} \\quad, 0 \\leq J(X, Y) \\leq 1\n","J(X,Y)= \n","∣XUY∣\n","∣X∩Y∣\n","​\n"," = \n","∣X∣+∣Y∣−∣X∩Y∣\n","∣X∩Y∣\n","​\n"," ,0≤J(X,Y)≤1"],"metadata":{"id":"0Zf4Vr5GUC_r"}},{"cell_type":"markdown","source":["이제 이 메소드를 활용해 어떻게 데이터셋을 추가로 가공하는지 살펴봅시다."],"metadata":{"id":"ngqZuLfyUDCX"}},{"cell_type":"code","source":["def _intersect(box_a, box_b):\n","    A = tf.shape(box_a)[0]\n","    B = tf.shape(box_b)[0]\n","    max_xy = tf.minimum(\n","        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n","        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n","    min_xy = tf.maximum(\n","        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n","        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n","    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n","    return inter[:, :, 0] * inter[:, :, 1]\n","\n","print('슝=3')"],"metadata":{"id":"5jo24kOGULCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _jaccard(box_a, box_b):\n","    inter = _intersect(box_a, box_b)\n","    area_a = tf.broadcast_to(\n","        tf.expand_dims(\n","            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n","        tf.shape(inter))  # [A,B]\n","    area_b = tf.broadcast_to(\n","        tf.expand_dims(\n","            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n","        tf.shape(inter))  # [A,B]\n","    union = area_a + area_b - inter\n","    return inter / union  # [A,B]\n","\n","print('슝=3')"],"metadata":{"id":"HyGBu-FLUNBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["자카드 유사도를 계산하는 메소드가 준비되었습니다. 아래 encode_tf는 이를 이용해서 TFRecord 데이터셋의 라벨을 가공하는 메소드입니다. 내용을 정리하면 다음과 같습니다.\n","\n","* jaccard 메소드를 이용해 label의 ground truth bbox와 가장 overlap 비율이 높은 matched box를 구한다.\n","* `_encode_bbox` 메소드를 통해 bbox의 scale을 동일하게 보정한다.\n","* 전체 default box에 대해 일정 threshold 이상 overlap되는 ground truth bounding box 존재 여부(positive/negative)를 concat하여 새로운 label로 업데이트한다."],"metadata":{"id":"gyFNw0TBUDE7"}},{"cell_type":"code","source":["def _encode_bbox(matched, boxes, variances=[0.1, 0.2]):\n","    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - boxes[:, :2]\n","    g_cxcy /= (variances[0] * boxes[:, 2:])\n","    g_wh = (matched[:, 2:] - matched[:, :2]) / boxes[:, 2:]\n","    g_wh = tf.math.log(g_wh) / variances[1]\n","    g_wh = tf.where(tf.math.is_inf(g_wh), 0.0, g_wh)\n","    return tf.concat([g_cxcy, g_wh], 1)\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6p_nWNnkUQpU","executionInfo":{"status":"ok","timestamp":1666251299654,"user_tz":-540,"elapsed":2,"user":{"displayName":"김용식","userId":"14840865344892218183"}},"outputId":"9b5dcc41-5efb-4b18-c3b5-96f2dbce79b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"code","source":["def encode_tf(labels, boxes):\n","    match_threshold = 0.45\n","    boxes = tf.cast(boxes, tf.float32)\n","    bbox = labels[:, :4]\n","    conf = labels[:, -1]\n","   \n","    # jaccard index\n","    overlaps = _jaccard(bbox, boxes)\n","    best_box_overlap = tf.reduce_max(overlaps, 1)\n","    best_box_idx = tf.argmax(overlaps, 1, tf.int32)\n","\n","    best_truth_overlap = tf.reduce_max(overlaps, 0)\n","    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n","\n","    best_truth_overlap = tf.tensor_scatter_nd_update(\n","        best_truth_overlap, tf.expand_dims(best_box_idx, 1),\n","        tf.ones_like(best_box_idx, tf.float32) * 2.)\n","    best_truth_idx = tf.tensor_scatter_nd_update(\n","        best_truth_idx, tf.expand_dims(best_box_idx, 1),\n","        tf.range(tf.size(best_box_idx), dtype=tf.int32))\n","    # Scale Ground-Truth Boxes   \n","    matches_bbox = tf.gather(bbox, best_truth_idx)\n","    loc_t = _encode_bbox(matches_bbox, boxes)\n","    conf_t = tf.gather(conf, best_truth_idx)\n","    conf_t = tf.where(tf.less(best_truth_overlap, match_threshold), tf.zeros_like(conf_t), conf_t)\n","\n","    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)\n","\n","print('슝=3')"],"metadata":{"id":"Fdu7Pv5qUS0h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **load_dataset**\n","---"],"metadata":{"id":"_c5lm4HXUDHH"}},{"cell_type":"markdown","source":["위에서 구현한 두가지 메소드를 이전 스텝에서 생성한 tfrecord 데이터셋에 적용하여 SSD 학습을 위한 데이터셋을 생성하는 최종 메소드인 load_dataset 을 구현합니다."],"metadata":{"id":"q-kou7CwUDJi"}},{"cell_type":"markdown","source":["* `_transform_data` : augmemtation과 label을 encoding 하여 기존의 dataset을 변환하는 메소드\n","* `_parse_tfrecord` : TFRecord 에 `_transform_data`를 적용하는 함수 클로저 생성\n","* `load_tfrecord_dataset` : `tf.data.TFRecordDataset.map()`에 `_parse_tfrecord`을 적용하는 실제 데이터셋 변환 메인 메소드\n","* `load_dataset` : `load_tfrecord_dataset`을 통해 train, validation 데이터셋을 생성하는 최종 메소드"],"metadata":{"id":"ARK-NHeTTlYm"}},{"cell_type":"code","source":["def _transform_data(train, boxes):\n","    def transform_data(img, labels):\n","        img = tf.cast(img, tf.float32)\n","        if train:\n","            img, labels = _crop(img, labels)\n","            img = _pad_to_square(img)\n","\n","        img, labels = _resize(img, labels)\n","\n","        if train:\n","            img, labels = _flip(img, labels)\n","\n","        if train:\n","            img = _distort(img)\n","        labels = encode_tf(labels, boxes)        \n","        img = img/255.0\n","        return img, labels\n","    return transform_data\n","\n","print('슝=3')"],"metadata":{"id":"aYofABnbUmv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _parse_tfrecord(train, boxes):\n","    def parse_tfrecord(tfrecord):\n","        features = {\n","            'filename': tf.io.FixedLenFeature([], tf.string),\n","            'height': tf.io.FixedLenFeature([], tf.int64),\n","            'width': tf.io.FixedLenFeature([], tf.int64),\n","            'classes': tf.io.VarLenFeature(tf.int64),\n","            'x_mins': tf.io.VarLenFeature(tf.float32),\n","            'y_mins': tf.io.VarLenFeature(tf.float32),\n","            'x_maxes': tf.io.VarLenFeature(tf.float32),\n","            'y_maxes': tf.io.VarLenFeature(tf.float32),\n","            'difficult':tf.io.VarLenFeature(tf.int64),\n","            'image_raw': tf.io.FixedLenFeature([], tf.string),\n","           }\n","\n","        parsed_example = tf.io.parse_single_example(tfrecord, features)\n","        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n","\n","        width = tf.cast(parsed_example['width'], tf.float32)\n","        height = tf.cast(parsed_example['height'], tf.float32)\n","\n","        labels = tf.sparse.to_dense(parsed_example['classes'])\n","        labels = tf.cast(labels, tf.float32)\n","\n","        labels = tf.stack(\n","            [tf.sparse.to_dense(parsed_example['x_mins']),\n","             tf.sparse.to_dense(parsed_example['y_mins']),\n","             tf.sparse.to_dense(parsed_example['x_maxes']),\n","             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n","\n","        img, labels = _transform_data(train, boxes)(img, labels)\n","\n","        return img, labels\n","    return parse_tfrecord\n","\n","print('슝=3')"],"metadata":{"id":"tPLM6dojUomR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_tfrecord_dataset(tfrecord_name, train=True, boxes=None, buffer_size=1024):\n","    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n","    raw_dataset = raw_dataset.cache()\n","    if train:\n","        raw_dataset = raw_dataset.repeat()\n","        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n","\n","    dataset = raw_dataset.map(_parse_tfrecord(train, boxes), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    return dataset\n","\n","print('슝=3')"],"metadata":{"id":"JsSIojV5UqYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataset(boxes, train=True, buffer_size=1024):\n","    if train:\n","        dataset = load_tfrecord_dataset(\n","            tfrecord_name=TRAIN_TFRECORD_PATH,\n","            train=train,\n","            boxes=boxes,\n","            buffer_size=buffer_size)\n","    else:\n","        dataset = load_tfrecord_dataset(\n","            tfrecord_name=VALID_TFRECORD_PATH,\n","            train=train,\n","            boxes=boxes,\n","            buffer_size=buffer_size)\n","    return dataset\n","\n","print('슝=3')"],"metadata":{"id":"jZBDV78AUsaI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["상당히 긴 데이터 준비 과정이 끝났습니다."],"metadata":{"id":"znKZ_7IzTlbJ"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"mmqQar7tTldl"}},{"cell_type":"markdown","source":["---\n","## **14-7. 모델 학습(2) train** 🔒| 60분\n","\n","---"],"metadata":{"id":"AKFUeM1bUyc_"}},{"cell_type":"markdown","source":["### **Learning rate scheduler**\n","---"],"metadata":{"id":"rhV3YI4AUyZx"}},{"cell_type":"markdown","source":["본격적으로 train에 들어가기 전에 2가지 더 준비해야 할 게 있습니다. 그중 하나는 Learning rate scheduler 입니다. 이번에는 초기시점에 WarmUp부분을 도입해 learning rate가 천천히 증가할 수 있도록 학습 스텝에 따라 다른 Learning Rate이 적용될 수 있도록 하겠습니다. PiecewiseConstantWarmUpDecay정도의 이름으로 만들어 사용할게요."],"metadata":{"id":"DrFwlkgaUyUv"}},{"cell_type":"code","source":["class PiecewiseConstantWarmUpDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, boundaries, values, warmup_steps, min_lr, name=None):\n","        super(PiecewiseConstantWarmUpDecay, self).__init__()\n","\n","        if len(boundaries) != len(values) - 1:\n","            raise ValueError(\n","                    \"The length of boundaries should be 1 less than the\"\n","                    \"length of values\")\n","\n","        self.boundaries = boundaries\n","        self.values = values\n","        self.name = name\n","        self.warmup_steps = warmup_steps\n","        self.min_lr = min_lr\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n","            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n","            pred_fn_pairs = []\n","            warmup_steps = self.warmup_steps\n","            boundaries = self.boundaries\n","            values = self.values\n","            min_lr = self.min_lr\n","\n","            pred_fn_pairs.append(\n","                (step <= warmup_steps,\n","                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n","            pred_fn_pairs.append(\n","                (tf.logical_and(step <= boundaries[0],\n","                                step > warmup_steps),\n","                 lambda: tf.constant(values[0])))\n","            pred_fn_pairs.append(\n","                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n","\n","            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n","                                    values[1:-1]):\n","                pred = (step > low) & (step <= high)\n","                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n","\n","            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n","                           exclusive=True)\n","\n","print('슝=3')"],"metadata":{"id":"QEgL6SD_U9q3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`PiecewiseConstantWarmUpDecay`를 편하게 사용할 수 있도록 함수를 만들어 둡니다."],"metadata":{"id":"NgFrIaMVUySJ"}},{"cell_type":"code","source":["def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n","                      warmup_steps=0., min_lr=0.,\n","                      name='MultiStepWarmUpLR'):\n","    assert warmup_steps <= lr_steps[0]\n","    assert min_lr <= initial_learning_rate\n","    lr_steps_value = [initial_learning_rate]\n","    for _ in range(len(lr_steps)):\n","        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n","    return PiecewiseConstantWarmUpDecay(\n","        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n","        min_lr=min_lr)\n","\n","print('슝=3')"],"metadata":{"id":"u9LZMLdqVAvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Hard negative mining**\n","---"],"metadata":{"id":"cqI_xQ2tUyPd"}},{"cell_type":"markdown","source":["Object Detection 모델 학습시 자주 사용되는 Hard negative mining이라는 기법이 있습니다. 학습과정에서 label은 negative인데 confidence가 높게 나오는 샘플을 재학습하면 positive와 negative의 모호한 경계선상에 분포한 false negative 오류에 강해진다는 장점이 있습니다. 실제로 confidence가 높은 샘플을 모아 training을 다시 수행하기보다는, 그런 샘플들에 대한 loss만 따로 모아 계산해주는 방식으로 반영할 수 있습니다."],"metadata":{"id":"TrovmmhhUyKZ"}},{"cell_type":"markdown","source":["Object Detection에서는 물체 영역보다 배경 영역이 훨씬 많기 십상이므로 negative 비율을 positive의 3배가 되도록 설정해 주었습니다."],"metadata":{"id":"CcNVJUQuUyHz"}},{"cell_type":"markdown","source":["아래 구현된 `hard_negative_mining` 메소드와, 이 메소드를 통해 얻은 샘플을 통해 얻은 localization loss를 기존의 classification loss에 추가로 반영하는 `MultiBoxLoss` 계산 메소드를 확인해 주세요."],"metadata":{"id":"KqT6_OruUyFh"}},{"cell_type":"code","source":["def hard_negative_mining(loss, class_truth, neg_ratio):\n","    pos_idx = class_truth > 0\n","    num_pos = tf.math.reduce_sum(tf.cast(pos_idx, tf.int32), axis=1)\n","    num_neg = num_pos * neg_ratio\n","\n","    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n","    rank = tf.argsort(rank, axis=1)\n","    neg_idx = rank < tf.expand_dims(num_neg, 1)\n","\n","    return pos_idx, neg_idx\n","\n","print('슝=3')"],"metadata":{"id":"DHNXq4LqVIay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def MultiBoxLoss(num_class, neg_pos_ratio=3.0):\n","    def multi_loss(y_true, y_pred):\n","        num_batch = tf.shape(y_true)[0]\n","        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n","        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n","\n","        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","        temp_loss = cross_entropy(class_truth, class_pred)\n","        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n","\n","        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n","        loss_class = cross_entropy(\n","            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n","            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n","        \n","        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n","        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n","\n","        num_pos = tf.math.reduce_sum(tf.cast(pos_idx, tf.float32))\n","\n","        loss_class = loss_class / num_pos\n","        loss_loc = loss_loc / num_pos\n","        return loss_loc, loss_class\n","\n","    return multi_loss\n","\n","print('슝=3')"],"metadata":{"id":"5F3TCB41VJ0j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Training**\n","---"],"metadata":{"id":"tSPisgcjUyDY"}},{"cell_type":"markdown","source":["이제 본격적으로 모델 학습을 진행하겠습니다.\n","\n","Default box를 만들고 데이터셋을 준비합니다."],"metadata":{"id":"YGTXkCLmUyAo"}},{"cell_type":"code","source":["boxes = default_box()\n","train_dataset = load_dataset(boxes, train=True)\n","\n","print('슝=3')"],"metadata":{"id":"ukYfkPCUVOe-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델을 만들고 살펴봅시다. 그림으로 표시되는 모델 구조는 크기가 큰 편이니 저장된 이미지 파일을 따로 열어 확인하는 것을 추천합니다. 'model.png'로 저장됩니다."],"metadata":{"id":"Hi5ERdSsUx-V"}},{"cell_type":"code","source":["model = SsdModel()\n","model.summary()\n","tf.keras.utils.plot_model(\n","    model, \n","    to_file=os.path.join(os.getcwd(), 'model.png'),\n","    show_shapes=True, \n","    show_layer_names=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"gMmHDgEhVTIe","executionInfo":{"status":"error","timestamp":1666251569700,"user_tz":-540,"elapsed":312,"user":{"displayName":"김용식","userId":"14840865344892218183"}},"outputId":"ecfc618d-d913-48d2-fafd-2d3c4090cbf3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-bfc468229151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSsdModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m tf.keras.utils.plot_model(\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SsdModel' is not defined"]}]},{"cell_type":"markdown","source":["나머지 학습에 필요한 요소들도 생성해 줍니다."],"metadata":{"id":"zpoCaUUmUx8M"}},{"cell_type":"code","source":["steps_per_epoch = DATASET_LEN // BATCH_SIZE\n","learning_rate = MultiStepWarmUpLR(\n","    initial_learning_rate=1e-2,\n","    lr_steps=[e*steps_per_epoch for e in [50, 70]],\n","    lr_rate=0.1,\n","    warmup_steps=5*steps_per_epoch,\n","    min_lr=1e-4\n",")\n","optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n","\n","multi_loss = MultiBoxLoss(len(IMAGE_LABELS), neg_pos_ratio=3)\n","\n","print('슝=3')"],"metadata":{"id":"SDLhkSBcVh2g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 학습이 진행될 함수를 작성할 차례입니다.<br>\n","`tf.GradientTape()`을 사용하는 방식에 익숙해 지셨나요? 아직 어렵다면 아래 링크를 참고하세요!<br>"],"metadata":{"id":"kEHzyq5oUx3N"}},{"cell_type":"markdown","source":["* [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n","* [Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)"],"metadata":{"id":"lM-32mAwWGwG"}},{"cell_type":"markdown","source":["한 스텝이 학습되는 함수를 아래처럼 작성합니다."],"metadata":{"id":"QXSE3SEQUx1O"}},{"cell_type":"code","source":["@tf.function\n","def train_step(inputs, labels):\n","    with tf.GradientTape() as tape:\n","        predictions = model(inputs, training=True)\n","        losses = {}\n","        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n","        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n","        total_loss = tf.add_n([l for l in losses.values()])\n","\n","    grads = tape.gradient(total_loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    return total_loss, losses\n","\n","print('슝=3')"],"metadata":{"id":"qfQxnAHSWLIs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래에서 본격적으로 train을 시작합니다. 100 epochs 이상 학습시켜야 좋은 성능을 보이지만 1 epoch 만 학습해 확인해 봅시다."],"metadata":{"id":"8cthgJUSUxyT"}},{"cell_type":"code","source":["EPOCHS = 1\n","\n","for epoch in range(0, EPOCHS):\n","    for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n","        load_t0 = time.time()\n","        total_loss, losses = train_step(inputs, labels)\n","        load_t1 = time.time()\n","        batch_time = load_t1 - load_t0\n","        print(f\"\\rEpoch: {epoch + 1}/{EPOCHS} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n","\n","    filepath = os.path.join(CHECKPOINT_PATH, f'weights_epoch_{(epoch + 1):03d}.h5')\n","    model.save_weights(filepath)"],"metadata":{"id":"mvfnhTxbWN8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"If0uRh4NUxv1"}},{"cell_type":"markdown","source":["---\n","## **14-8. inference(1) NMS** 🔒| 20분\n","\n","---"],"metadata":{"id":"5j_yFNcqWR4f"}},{"cell_type":"markdown","source":["### **NMS 구현하기**\n","---"],"metadata":{"id":"VC7SCfy1WR16"}},{"cell_type":"markdown","source":["Grid cell을 사용하는 Object detection의 inference 단계에서 하나의 object가 여러 개의 default box에 걸쳐져 있을 때 IoU가 가장 높은 default box를 선택하는 NMS(non-max suppression)이 필요합니다. 아래 코드를 확인해 주세요."],"metadata":{"id":"wOEq4oM1WRzt"}},{"cell_type":"code","source":["def compute_nms(boxes, scores, nms_threshold=0.4, limit=200):\n","    if boxes.shape[0] == 0:\n","        return tf.constant([], dtype=tf.int32)\n","    selected = [0]\n","    idx = tf.argsort(scores, direction='DESCENDING')\n","    idx = idx[:limit]\n","    boxes = tf.gather(boxes, idx)\n","\n","    iou = _jaccard(boxes, boxes)\n","\n","    while True:\n","        row = iou[selected[-1]]\n","        next_indices = row <= nms_threshold\n","\n","        iou = tf.where(\n","            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n","            tf.ones_like(iou, dtype=tf.float32),\n","            iou\n","        )\n","\n","        if not tf.math.reduce_any(next_indices):\n","            break\n","\n","        selected.append(tf.argsort(\n","            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n","\n","    return tf.gather(idx, selected)\n","\n","print('슝=3')"],"metadata":{"id":"GwFS--ITWZt_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NMS를 통해 겹쳐진 box를 하나로 줄일 수 있게 되었다면, 이제 모델의 예측 결과를 해석해주는 함수를 작성합니다."],"metadata":{"id":"xonK3BVqWRxK"}},{"cell_type":"markdown","source":["아래 함수에서는 모델의 예측 결과를 디코딩해서 예측 확률을 토대로 NMS를 통해 최종 box와 score 결과를 만들어 줍니다."],"metadata":{"id":"bQKoCUsHWRuu"}},{"cell_type":"code","source":["def decode_bbox_tf(predicts, boxes, variances=[0.1, 0.2]):\n","    centers = boxes[:, :2] + predicts[:, :2] * variances[0] * boxes[:, 2:]\n","    sides = boxes[:, 2:] * tf.math.exp(predicts[:, 2:] * variances[1])\n","    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUubsknQWet-","executionInfo":{"status":"ok","timestamp":1666251880463,"user_tz":-540,"elapsed":4,"user":{"displayName":"김용식","userId":"14840865344892218183"}},"outputId":"19b714c0-28cc-4483-f0b8-afbf034a80d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"code","source":["def parse_predict(predictions, boxes):\n","    label_classes = IMAGE_LABELS\n","\n","    bbox_predictions, confidences = tf.split(predictions[0], [4, -1], axis=-1)\n","    boxes = decode_bbox_tf(bbox_predictions, boxes)\n","\n","    scores = tf.math.softmax(confidences, axis=-1)\n","\n","    out_boxes = []\n","    out_labels = []\n","    out_scores = []\n","\n","    for c in range(1, len(label_classes)):\n","        cls_scores = scores[:, c]\n","\n","        score_idx = cls_scores > 0.5\n","\n","        cls_boxes = boxes[score_idx]\n","        cls_scores = cls_scores[score_idx]\n","\n","        nms_idx = compute_nms(cls_boxes, cls_scores)\n","\n","        cls_boxes = tf.gather(cls_boxes, nms_idx)\n","        cls_scores = tf.gather(cls_scores, nms_idx)\n","\n","        cls_labels = [c] * cls_boxes.shape[0]\n","\n","        out_boxes.append(cls_boxes)\n","        out_labels.extend(cls_labels)\n","        out_scores.append(cls_scores)\n","\n","    out_boxes = tf.concat(out_boxes, axis=0)\n","    out_scores = tf.concat(out_scores, axis=0)\n","\n","    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n","    classes = np.array(out_labels)\n","    scores = out_scores.numpy()\n","\n","    return boxes, classes, scores\n","\n","print('슝=3')"],"metadata":{"id":"YUg2cXJHWgsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"8cG391RqWRsO"}},{"cell_type":"markdown","source":["---\n","## **14-9. inference(2) 사진에서 얼굴 찾기** 🔒| 30분\n","\n","---"],"metadata":{"id":"YTzrfQcuWRp6"}},{"cell_type":"markdown","source":["### **사진에서 여러개의 얼굴을 찾아보자.**\n","---"],"metadata":{"id":"4_rfIsRZWRnS"}},{"cell_type":"markdown","source":["이제 다 왔습니다. SSD 모델을 통해 우리는 Multi-face detection 기능을 확보했습니다.\n","얼마나 잘 해내는지 확인해 보도록 합시다."],"metadata":{"id":"Tvexo28QWRk_"}},{"cell_type":"markdown","source":["우선 입력할 이미지의 비율이 달라도 모델이 잘 작동할 수 있도록 이미지 패딩을 추가/제거해주는 함수를 만들어 줍니다."],"metadata":{"id":"u5K_KUDEWRis"}},{"cell_type":"code","source":["def pad_input_image(img, max_steps):\n","    img_h, img_w, _ = img.shape\n","\n","    img_pad_h = 0\n","    if img_h % max_steps > 0:\n","        img_pad_h = max_steps - img_h % max_steps\n","\n","    img_pad_w = 0\n","    if img_w % max_steps > 0:\n","        img_pad_w = max_steps - img_w % max_steps\n","\n","    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n","    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n","                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n","    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n","\n","    return img, pad_params\n","\n","print('슝=3')"],"metadata":{"id":"uB-7_IoJWt0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recover_pad(boxes, pad_params):\n","    img_h, img_w, img_pad_h, img_pad_w = pad_params\n","    box = np.reshape(boxes[0], [-1, 2, 2]) * [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n","    boxes[0] = np.reshape(box, [-1, 4])\n","    return boxes\n","\n","print('슝=3')"],"metadata":{"id":"_WBMuIYhWvc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래는 최종 결과를 시각화 할 수 있도록 이미지에 box를 그려주는 함수입니다. box 사각형과 함께 확률도 표시해 줍시다."],"metadata":{"id":"Qdxo5FvwWRgU"}},{"cell_type":"code","source":["def draw_box_on_face(img, boxes, classes, scores, box_index, class_list):\n","    img_height = img.shape[0]\n","    img_width = img.shape[1]\n","\n","    x_min = int(boxes[box_index][0] * img_width)\n","    y_min = int(boxes[box_index][1] * img_height)\n","    x_max = int(boxes[box_index][2] * img_width)\n","    y_max = int(boxes[box_index][3] * img_height)\n","\n","    if classes[box_index] == 1:\n","        color = (0, 255, 0)\n","    else:\n","        color = (0, 0, 255)\n","    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n","    \n","    if len(scores) > box_index :\n","        score = \"{:.4f}\".format(scores[box_index])\n","        class_name = class_list[classes[box_index]]\n","        label = '{} {}'.format(class_name, score)\n","        position = (x_min, y_min - 4) \n","        cv2.putText(img, label, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n","\n","print('슝=3')"],"metadata":{"id":"mKA13NPJWy7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래 코드를 실행하여 클라우드에 저장된 테스트용 이미지의 결과를 확인해보세요."],"metadata":{"id":"yS8PI-dsWRdv"}},{"cell_type":"code","source":["filepath = os.path.join(PROJECT_PATH, 'checkpoints', 'weights_epoch_008.h5')\n","model.load_weights(filepath)\n","\n","TEST_IMAGE_PATH = os.path.join(PROJECT_PATH, 'image_people.png')\n","\n","img_raw = cv2.imread(TEST_IMAGE_PATH)\n","img_raw = cv2.resize(img_raw, (IMAGE_WIDTH, IMAGE_HEIGHT))\n","img = np.float32(img_raw.copy())\n","\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","img, pad_params = pad_input_image(img, max_steps=max(BOX_STEPS))\n","img = img / 255.0\n","\n","boxes = default_box()\n","boxes = tf.cast(boxes, tf.float32)\n","\n","predictions = model.predict(img[np.newaxis, ...])\n","\n","pred_boxes, labels, scores = parse_predict(predictions, boxes)\n","pred_boxes = recover_pad(pred_boxes, pad_params)\n","\n","for box_index in range(len(pred_boxes)):\n","    draw_box_on_face(img_raw, pred_boxes, labels, scores, box_index, IMAGE_LABELS)\n","\n","plt.imshow(cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB))\n","plt.show()"],"metadata":{"id":"3Z-gAxkLW2Ii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["어떤가요? 결과가 잘 나오시나요? 하이퍼파라미터를 약간 조정하고 학습을 조금 더 돌려서 저는 이런 결과를 얻었습니다."],"metadata":{"id":"KnOGY9cbWRX7"}},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"9Um3GjtVWRU6"}},{"cell_type":"markdown","source":["---\n","## **14-10. 프로젝트 : 스티커를 붙여주자** 🔒| 180분\n","\n","---"],"metadata":{"id":"7p0gQjuaW_Og"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ib3sLYuDW_MZ"}},{"cell_type":"markdown","source":["드디어 딥러닝 모델을 이용해 우리의 스티커앱을 개선해서, 이미지 속 다수의 사람에게 스티커를 붙여줄 수 있게 되었습니다. 위 그림의 예시처럼 말이죠."],"metadata":{"id":"X1ecRU2AW_KB"}},{"cell_type":"markdown","source":["여러분들도 여러분들의 작품을 통해 이 작업을 본격적으로 수행해 봅시다. 첫 번째 스텝에서 제공해 드린 프로젝트 코드를 적극적으로 활용하셔도 좋습니다."],"metadata":{"id":"5najPJjeW_Hi"}},{"cell_type":"markdown","source":["라이브러리 버전 확인"],"metadata":{"id":"tG6BpwzTW_Cy"}},{"cell_type":"markdown","source":["사용할 주요 라이브러리 버전을 확인해봅니다."],"metadata":{"id":"8gPMUlSoW-__"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","\n","print(tf.__version__)\n","print(np.__version__)\n","print(cv2.__version__)"],"metadata":{"id":"PGa_SVS3XNIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 1. 스티커 구하기 혹은 만들기"],"metadata":{"id":"HcJjRR00W-7A"}},{"cell_type":"markdown","source":["왕관 또는 고양이 수염 등을 구하거나 혹은 다양한 아이디어의 스티커를 만들어 볼 수 있을 것입니다."],"metadata":{"id":"D0YzuhAvW-4b"}},{"cell_type":"markdown","source":["Step 2. SSD 모델을 통해 얼굴 bounding box 찾기"],"metadata":{"id":"Hfg1p0m1W-16"}},{"cell_type":"markdown","source":["우리는 실습 코드를 진행하며 필요한 모델을 이미 생성해 왔을 것입니다. 잘 훈련된 해당 모델을 통해 적절한 얼굴 bounding box를 찾아내 봅시다. inference.py 코드를 적극적으로 참고해 보시기를 권합니다."],"metadata":{"id":"I2-PQIXjW-zX"}},{"cell_type":"markdown","source":["Step 3. dlib 을 이용한 landmark 찾기 (선택사항)"],"metadata":{"id":"3fRnvJN5W-w8"}},{"cell_type":"markdown","source":["붙이려는 이미지에 따라 face landmark를 찾아야 할 수도 있습니다. 검출된 bounding box에 dlib을 적용해 face landmark를 찾을 수 있을 것입니다.\n","dlib를 이용하려는 경우, inference.py 에서 show_image 메소드를 사용한 부분을 적절히 수정해야 가능할 것입니다."],"metadata":{"id":"HWshhsKKXVm_"}},{"cell_type":"markdown","source":["Step 4. 스티커 합성 사진 생성하기"],"metadata":{"id":"VU05PZz6XVqB"}},{"cell_type":"markdown","source":["여러분들이 선택한 인물사진에 스티커를 합성해 봅시다. 이미지에 너무 많은 사람 얼굴이 포함되어 있거나, 검출된 얼굴이 너무 작아서 스티커 합성이 어울리지 않으면 적당하지 않겠죠? 3~5명 정도의 얼굴이 포함된 적당한 사진을 선택해 주세요.\n","\n","생성된 이미지를 프로젝트 코드와 함께 제출해 주세요~\n","\n","그동안 수고 많으셨습니다!!!\n"],"metadata":{"id":"3H9PzR0uXVsJ"}},{"cell_type":"markdown","source":["---\n","## **14-11. 프로젝트 제출** 🔒\n","\n","---"],"metadata":{"id":"66Ze-C5tXVut"}},{"cell_type":"markdown","source":["### **루브릭**\n","\n","아래의 기준을 바탕으로 프로젝트를 평가합니다."],"metadata":{"id":"5JfmE5spfnW0"}},{"cell_type":"markdown","source":["|평가문항|상세기준|\n","|:---|:---|\n","|1. multiface detection을 위한 widerface 데이터셋의 전처리가 적절히 진행되었다.|tfrecord 생성, augmentation, prior box 생성 등의 과정이 정상적으로 진행되었다.|\n","|2. SSD 모델이 안정적으로 학습되어 multiface detection이 가능해졌다.|inference를 통해 정확한 위치의 face bounding box를 detect한 결과이미지가 제출되었다.|\n","|3. 이미지 속 다수의 얼굴에 스티커가 적용되었다.|이미지 속 다수의 얼굴의 적절한 위치에 스티커가 적용된 결과이미지가 제출되었다.|"],"metadata":{"id":"iWXon9zefnZe"}},{"cell_type":"markdown","source":["### **프로젝트 업로드 (URL)**"],"metadata":{"id":"BVRu8GixfnpF"}},{"cell_type":"markdown","source":["GitHub 또는 Google URL을 입력하신 후 하단의 [성취하기] 버튼을 눌러주세요."],"metadata":{"id":"PbLsb3hMfnt2"}},{"cell_type":"markdown","source":["https://github.com/example/url"],"metadata":{"id":"_YFySoNVySFb"}},{"cell_type":"markdown","source":[],"metadata":{"id":"7IEld3AfXVzc"}},{"cell_type":"markdown","source":[],"metadata":{"id":"3bemVTatW-ur"}},{"cell_type":"markdown","source":[],"metadata":{"id":"4JrzIZRRW-r9"}},{"cell_type":"code","source":[],"metadata":{"id":"O_1RnVY4Uw_7"},"execution_count":null,"outputs":[]}]}