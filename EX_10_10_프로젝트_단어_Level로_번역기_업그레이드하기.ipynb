{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX_10-10. 프로젝트 : 단어 Level로 번역기 업그레이드하기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNlFlhT5AfQjGu/o5WSpORS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivrm/ai/blob/main/EX_10_10_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%EB%8B%A8%EC%96%B4_Level%EB%A1%9C_%EB%B2%88%EC%97%AD%EA%B8%B0_%EC%97%85%EA%B7%B8%EB%A0%88%EC%9D%B4%EB%93%9C%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORATION_BS2\n",
        "\n",
        "---\n",
        "# 10. 번역기를 만들어보자\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_mVU96oete5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 10-10. 프로젝트 : 단어 Level로 번역기 업그레이드하기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LMsAw3ZL8NIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "프로젝트를 진행하기 전에 주요 라이브러리 버전을 확인해 보죠."
      ],
      "metadata": {
        "id": "L2irbFOL8Qbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "WLgMNiwB8LRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0bc9db-a2a7-47f1-949a-72668fca708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습에서 구현한 번역기는 글자 단위(Character-level)에서 구현된 번역기였습니다.<br>\n",
        "하지만 실제 번역기의 경우에는 글자 단위가 아니라 단어 단위(Word-level)에서 구현되는 것이 좀 더 보편적입니다.<br>"
      ],
      "metadata": {
        "id": "xT9U1pfc8Ucu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 임베딩 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시켜보겠습니다.<br>\n",
        "하지만, 단어 단위로 할 경우에는 단어의 개수가 글자 단위로 했을 경우와 비교하여 단어장의 크기(Vocabulary) 크기도 커지고, 학습 속도도 좀 더 느려집니다.<br>\n",
        "**학습과 테스트 시의 원활한 진행을 위해서 데이터에서 상위 33,000개의 샘플만 사용해주세요.**<br>"
      ],
      "metadata": {
        "id": "yfg85Mgw8UTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는 용도로 사용합니다.**"
      ],
      "metadata": {
        "id": "-Wf9n-MV8UQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "K8O-_usu8ULy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "글자 단위가 아닌 단어 단위의 번역기를 하기 위해서는 글자 단위에서는 신경쓰지 않았던 몇 가지 추가적인 전처리가 필요합니다."
      ],
      "metadata": {
        "id": "fvMRuPTWCmoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 구두점(Punctuation)을 단어와 분리해주세요."
      ],
      "metadata": {
        "id": "skPQPcqrCt_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로 영어권 언어의 경우에는 띄어쓰기 단위로 단어를 분리합니다.<br>\n",
        "토큰화(Tokenization)이라고도 불리는 이 작업은 어디서부터 어디까지가 하나의 단어인지를 구분하는 작업인데요.<br>\n",
        "그런데 띄어쓰기를 해주기 전에 구두점을 분리하는 작업이 필요할 때가 있습니다.<br>\n",
        "<br>\n",
        "예를 들어서 'he is a good boy!'라는 문장이 있을 때, 이를 띄어쓰기 단위로 토큰화한다면<br>\n",
        "['he', 'is', 'a', 'good', 'boy!']가 됩니다.<br>\n",
        "<br>\n",
        "그런데 실제로 !는 boy와 붙어있는 한 단어가 아니므로<br>\n",
        "좀 더 올바른 전처리는 ['he', 'is', 'a', 'good', 'boy', '!']가 맞습니다.<br>\n",
        "<br>\n",
        "!나 ? 또는 온점과 같은 특수문자들을 구두점(punctuation)이라고 부릅니다.<br>\n",
        "이들을 토큰화하기 전에 단어와 미리 분리시켜주세요!<br>\n",
        "<br>\n",
        "분리 전 : he is a Good boy!<br>\n",
        "분리 후 : he is a Good boy !"
      ],
      "metadata": {
        "id": "5ard8eFe8jQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 소문자로 바꿔주세요."
      ],
      "metadata": {
        "id": "_2U1f5lMDjsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기계가 보기에는 스펠링이 같더라도 대문자로 된 단어와 소문자로 된 단어는 서로 다른 단어입니다.<br>\n",
        "예를 들어 'Good'과 'good'은 기계가 보기에는 다른 단어입니다.<br>\n",
        "그래서 모든 문장에 대해서 전부 영어로 바꿔주는 작업을 하겠습니다.<br>\n",
        "<br>\n",
        "변환 전 : he is a Good boy !<br>\n",
        "변환 후 : he is a good boy !"
      ],
      "metadata": {
        "id": "FVobdgeC8lH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. 띄어쓰기 단위로 토큰를 수행하세요."
      ],
      "metadata": {
        "id": "izVz6qUREDVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "띄어쓰기 단위로 토큰화를 수행해서 단어를 분리하는 작업을 해주세요.<br>\n",
        "기계는 이렇게 분리된 토큰들을 각각 하나의 단어로 인식할 수 있게 됩니다.<br>\n",
        "<br>\n",
        "토큰화 전 : 'he is a good boy !'<br>\n",
        "토큰화 후 : ['he', 'is', 'a', 'good', 'boy', '!']"
      ],
      "metadata": {
        "id": "MYMurpJh8mar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZsyTn2WT6FSP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "rekhTNiX6N6K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19만개 중 33000개만 사용\n",
        "num_samples = 33000"
      ],
      "metadata": {
        "id": "TiUxeIGl6N9-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# 전처리\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "wamSLWrI6OC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic6DuYKW6OGC",
        "outputId": "770e0852-c630-4888-d85b-375efea8feff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8T9CGnS18nnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "글자 단위 번역기를 구현할 때와 마찬가지로 디코더의 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰인 가 필요합니다. 그리고 교사 강요를 수행할 때, 디코더의 실제값이 되는 디코더의 레이블 시퀀스에는 종료를 의미하는 종료 토큰 가 필요합니다.<br>\n",
        "<br>\n",
        "예를 들어 번역 문장이 Courez!이었다고 한다면, Step 1을 거친 후에는 다음과 같은 결과를 얻습니다.<br>\n",
        "<br>\n",
        "Step 1을 수행한 후 : ['courez', '!']<br>\n",
        "<br>\n",
        "이 문장에 대해서 각각 디코더의 입력 시퀀스와 레이블 시퀀스를 만들면 다음과 같습니다.<br>\n",
        "<br>\n",
        "입력 시퀀스 : ['', 'courez', '!']<br>\n",
        "레이블 시퀀스 : ['courez', '!', ']<br>\n",
        "<br>\n",
        "참고로 Step 2가 반드시 Step 1이 끝난 후에 이루어질 필요는 없습니다!<br>\n",
        "<br>\n",
        "Step 1을 수행하는 중간에 수행해도 상관없습니다.<br>"
      ],
      "metadata": {
        "id": "r4EbrSfB8sTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "RbTnecvb6OI_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5개만 샘플로 보기\n",
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPFIN3nZ6OL3",
        "outputId": "22890f40-52ff-46c9-ef78-f92c7a51e722"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OLwaAvWR8uK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥 러닝 모델은 각 단어를 텍스트가 아닌 숫자를 처리합니다.<br>\n",
        "케라스 토크나이저를 사용해서 각 단어를 고유한 정수로 바꿔보세요.<br>\n",
        "케라스 토크나이저의 사용법은<br>\n",
        "아래의 링크에서 2. 케라스(Keras)의 텍스트 전처리 에 설명되어져 있습니다.<br>\n",
        "\n",
        "---\n",
        "▣ 위키독스<br>\n",
        "https://wikidocs.net/31766\n",
        "\n",
        "---\n",
        "\n",
        "위 링크의 가이드를 통해서 영어와 프랑스어에 대한 토크나이저를 각각 생성하고,<br>\n",
        "tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환해보세요.<br>"
      ],
      "metadata": {
        "id": "kYj7JSkf8xZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keras 의 Tokenizer를 이용하여 처리\n",
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "aVF-fRW06OOf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYAMhvXZ6OT4",
        "outputId": "6e3de1ee-16dc-4373-eb26-f767137fe12f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 8)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub-xEes06OWn",
        "outputId": "0fe5a650-689f-4162-8acf-338cd64d817a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4672, 프랑스어 단어 집합의 크기 : 8153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "vEMpxQYu6OZV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data shuffle\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJIvfmDQ6cfr",
        "outputId": "3e4ef830-95f2-4d7c-d8ce-e41a2b1c0b42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [ 3275 15449 28895 ...  2165 14495 17123]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "sejWOAVk6cid"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkR7U4iJ6ck_",
        "outputId": "edd0a04a-c60e-4cf9-e7c2-d0375a9d8a40"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  15,    8,    9,  131, 1106,    1,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l4rGCG16csS",
        "outputId": "15db02d8-5386-4e90-8906-0d1df7286307"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  19,   5,  24, 241, 837,   1,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb7pDC-h6cxq",
        "outputId": "1a4ff479-5ecb-4cba-b3c0-31fa9291c330"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 19,   5,  24, 241, 837,   1,   3,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33000 개 중 10%인 3300개 를 테스트 데이터로 함."
      ],
      "metadata": {
        "id": "XWzpMTQFRhVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ZGWEQA6lor",
        "outputId": "578c5062-6723-4eb4-806e-8db2af569880"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "hII6upen6lmZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcupHC8w6lkF",
        "outputId": "0e879b6e-3a00-4b09-c612-87ea684bb4cd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 8)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 8)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 4. 임베딩 층(Embedding layer) 사용하기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V02QE_az885i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 입력이 되는 각 단어를 임베딩 층을 사용하여 벡터화하겠습니다.\n",
        "\n",
        "임베딩 층을 사용하는 방법과 그 설명에 대해서는 아래의 링크의\n",
        "\n",
        "1. 케라스 임베딩 층(Keras Embedding layer) 를 참고하세요.\n",
        "\n",
        "---\n",
        "▣ 위키독스<br>\n",
        "https://wikidocs.net/33793\n",
        "\n",
        "---\n",
        "\n",
        "실제 번역기 구현을 위해서 사용할 수 있는 인코더 코드의 예시는 다음과 같습니다.\n",
        "\n",
        "이를 통해서 인코더와 디코더의 임베딩 층을 각각 구현해보세요."
      ],
      "metadata": {
        "id": "HSxrm8CW9CKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from tensorflow.keras.layers import Input, mbedding, Masking\n",
        "\n",
        "# 인코더에서 사용할 임베딩 층 사용 예시\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
        "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "```"
      ],
      "metadata": {
        "id": "pPgcp8lO9KQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만,<br>\n",
        "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 합니다!"
      ],
      "metadata": {
        "id": "R3vlQEfJ9RaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "metadata": {
        "id": "WGAyYgCh6lb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "8YnoT7WP6lhT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "zIy-otMs6leX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# 모델의 입력과 출력을 정의.\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "g4QsV2f56r8R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GaTi7BJ6r-4",
        "outputId": "b3c2d4cb-76ab-4e35-d4ab-b7c4c98e1241"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "233/233 [==============================] - 21s 38ms/step - loss: 3.4153 - acc: 0.6116 - val_loss: 2.0704 - val_acc: 0.6162\n",
            "Epoch 2/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.8730 - acc: 0.6788 - val_loss: 1.7596 - val_acc: 0.7375\n",
            "Epoch 3/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.6700 - acc: 0.7420 - val_loss: 1.6227 - val_acc: 0.7509\n",
            "Epoch 4/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 1.5444 - acc: 0.7558 - val_loss: 1.5087 - val_acc: 0.7602\n",
            "Epoch 5/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 1.4383 - acc: 0.7652 - val_loss: 1.4232 - val_acc: 0.7699\n",
            "Epoch 6/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.3508 - acc: 0.7798 - val_loss: 1.3417 - val_acc: 0.7880\n",
            "Epoch 7/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.2682 - acc: 0.7956 - val_loss: 1.2741 - val_acc: 0.7995\n",
            "Epoch 8/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.2017 - acc: 0.8048 - val_loss: 1.2217 - val_acc: 0.8079\n",
            "Epoch 9/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.1440 - acc: 0.8135 - val_loss: 1.1771 - val_acc: 0.8129\n",
            "Epoch 10/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.0914 - acc: 0.8201 - val_loss: 1.1373 - val_acc: 0.8177\n",
            "Epoch 11/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 1.0450 - acc: 0.8253 - val_loss: 1.1035 - val_acc: 0.8225\n",
            "Epoch 12/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.0029 - acc: 0.8303 - val_loss: 1.0781 - val_acc: 0.8261\n",
            "Epoch 13/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.9649 - acc: 0.8349 - val_loss: 1.0512 - val_acc: 0.8291\n",
            "Epoch 14/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.9309 - acc: 0.8382 - val_loss: 1.0281 - val_acc: 0.8323\n",
            "Epoch 15/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.8990 - acc: 0.8416 - val_loss: 1.0112 - val_acc: 0.8334\n",
            "Epoch 16/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.8700 - acc: 0.8451 - val_loss: 0.9916 - val_acc: 0.8353\n",
            "Epoch 17/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.8421 - acc: 0.8480 - val_loss: 0.9747 - val_acc: 0.8380\n",
            "Epoch 18/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.8162 - acc: 0.8508 - val_loss: 0.9612 - val_acc: 0.8388\n",
            "Epoch 19/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.7920 - acc: 0.8534 - val_loss: 0.9457 - val_acc: 0.8413\n",
            "Epoch 20/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.7680 - acc: 0.8559 - val_loss: 0.9323 - val_acc: 0.8422\n",
            "Epoch 21/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.7464 - acc: 0.8583 - val_loss: 0.9206 - val_acc: 0.8437\n",
            "Epoch 22/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.7238 - acc: 0.8609 - val_loss: 0.9086 - val_acc: 0.8448\n",
            "Epoch 23/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.7031 - acc: 0.8631 - val_loss: 0.8992 - val_acc: 0.8463\n",
            "Epoch 24/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6845 - acc: 0.8650 - val_loss: 0.8907 - val_acc: 0.8475\n",
            "Epoch 25/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6644 - acc: 0.8672 - val_loss: 0.8812 - val_acc: 0.8481\n",
            "Epoch 26/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6460 - acc: 0.8697 - val_loss: 0.8739 - val_acc: 0.8488\n",
            "Epoch 27/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6301 - acc: 0.8715 - val_loss: 0.8678 - val_acc: 0.8500\n",
            "Epoch 28/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6122 - acc: 0.8736 - val_loss: 0.8574 - val_acc: 0.8519\n",
            "Epoch 29/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5950 - acc: 0.8759 - val_loss: 0.8525 - val_acc: 0.8523\n",
            "Epoch 30/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.5794 - acc: 0.8778 - val_loss: 0.8464 - val_acc: 0.8532\n",
            "Epoch 31/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.5638 - acc: 0.8799 - val_loss: 0.8438 - val_acc: 0.8531\n",
            "Epoch 32/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.5490 - acc: 0.8817 - val_loss: 0.8362 - val_acc: 0.8542\n",
            "Epoch 33/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.5348 - acc: 0.8837 - val_loss: 0.8328 - val_acc: 0.8546\n",
            "Epoch 34/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5209 - acc: 0.8857 - val_loss: 0.8263 - val_acc: 0.8561\n",
            "Epoch 35/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5080 - acc: 0.8876 - val_loss: 0.8250 - val_acc: 0.8560\n",
            "Epoch 36/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4943 - acc: 0.8897 - val_loss: 0.8205 - val_acc: 0.8563\n",
            "Epoch 37/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4814 - acc: 0.8917 - val_loss: 0.8160 - val_acc: 0.8575\n",
            "Epoch 38/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4695 - acc: 0.8936 - val_loss: 0.8130 - val_acc: 0.8577\n",
            "Epoch 39/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4576 - acc: 0.8958 - val_loss: 0.8111 - val_acc: 0.8576\n",
            "Epoch 40/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.4465 - acc: 0.8976 - val_loss: 0.8081 - val_acc: 0.8590\n",
            "Epoch 41/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.4357 - acc: 0.8995 - val_loss: 0.8053 - val_acc: 0.8590\n",
            "Epoch 42/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.4248 - acc: 0.9014 - val_loss: 0.8040 - val_acc: 0.8598\n",
            "Epoch 43/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.4136 - acc: 0.9037 - val_loss: 0.8017 - val_acc: 0.8608\n",
            "Epoch 44/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.4037 - acc: 0.9053 - val_loss: 0.8015 - val_acc: 0.8606\n",
            "Epoch 45/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.3951 - acc: 0.9067 - val_loss: 0.7975 - val_acc: 0.8617\n",
            "Epoch 46/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3842 - acc: 0.9089 - val_loss: 0.7961 - val_acc: 0.8618\n",
            "Epoch 47/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3747 - acc: 0.9107 - val_loss: 0.7948 - val_acc: 0.8620\n",
            "Epoch 48/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3667 - acc: 0.9125 - val_loss: 0.7960 - val_acc: 0.8628\n",
            "Epoch 49/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3586 - acc: 0.9140 - val_loss: 0.7971 - val_acc: 0.8629\n",
            "Epoch 50/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3502 - acc: 0.9158 - val_loss: 0.7942 - val_acc: 0.8636\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1a4a726190>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 5. 모델 구현하기\n",
        "---"
      ],
      "metadata": {
        "id": "deYAKQfK9S5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "글자 단위 번역기에서 구현한 모델을 참고로 단어 단위 번역기의 모델을 완성시켜보세요!"
      ],
      "metadata": {
        "id": "WHdvAvJ09WXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더 설계 시작\n",
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# 수정된 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "1GBI6E9m6sBR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "6_jstEJP6sDy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 6. 모델 평가하기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LtEizpLU9X8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 단위 번역기에 대해서 훈련 데이터의 샘플과 테스트 데이터의 샘플에 대해서 번역 문장을 만들어보고 정답 문장과 번역 문장을 비교해보세요."
      ],
      "metadata": {
        "id": "MCS6jiyo9c7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "B3Bsnnv76sGg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBH9HwgJ6sJJ",
        "outputId": "cc2e6fe6-b447-4992-fe85-63e784aebb08"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : please step aside . \n",
            "정답문장 : veuillez vous mettre de cote . \n",
            "번역문장 : veuillez vous montrer . \n",
            "--------------------------------------------------\n",
            "입력문장 : i m trapped . \n",
            "정답문장 : je suis piegee . \n",
            "번역문장 : je suis piege . \n",
            "--------------------------------------------------\n",
            "입력문장 : you re precise . \n",
            "정답문장 : tu es precis . \n",
            "번역문장 : tu es precis . \n",
            "--------------------------------------------------\n",
            "입력문장 : please be polite . \n",
            "정답문장 : veuillez vous montrer polies . \n",
            "번역문장 : veuillez vous montrer polie . \n",
            "--------------------------------------------------\n",
            "입력문장 : we meant well . \n",
            "정답문장 : nos intentions etaient bonnes . \n",
            "번역문장 : nos intentions etaient bonnes . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "veuillez vous mettre de cote (옆에 서주세요)<br>\n",
        "veuillez vous montrer (자신을 보여주세요)<br>\n",
        "<br>\n",
        "je suis piegee (내가 갇혀있어)<br>\n",
        "je suis piege (덫에 걸렸어)<br>\n",
        "<br>\n",
        "tu es precis (당신은 정확합니다)<br>\n",
        "<br>\n",
        "veuillez vous montrer polies (예의를 지켜주세요)<br>\n",
        "veuillez vous montrer polie (예의를 지켜주세요)<br>\n",
        "<br>\n",
        "nos intentions etaient bonnes (우리의 의도는 좋았어)<br>"
      ],
      "metadata": {
        "id": "Xw3qSuG4McnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HctmZkzU6sLr",
        "outputId": "e5cc909b-e989-4652-eba2-ee792a10f3ce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : that house is big . \n",
            "정답문장 : cette maison est grande . \n",
            "번역문장 : c est la maison . \n",
            "--------------------------------------------------\n",
            "입력문장 : go home . \n",
            "정답문장 : rentre chez toi . \n",
            "번역문장 : va la maison ! \n",
            "--------------------------------------------------\n",
            "입력문장 : we know everything . \n",
            "정답문장 : nous savons tout . \n",
            "번역문장 : nous savons en ville ! \n",
            "--------------------------------------------------\n",
            "입력문장 : how tall you are ! \n",
            "정답문장 : que vous etes grandes ! \n",
            "번역문장 : comme vous etes grands ! \n",
            "--------------------------------------------------\n",
            "입력문장 : i am so exhausted ! \n",
            "정답문장 : je suis si epuise ! \n",
            "번역문장 : je suis si idiot . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cette maison est grande (이 집은 크다)<br>\n",
        "c est la maison (그것은 집이다)<br>\n",
        "<br>\n",
        "rentre chez toi (집에가)<br>\n",
        "va la maison (집에가)<br>\n",
        "<br>\n",
        "nous savons tout (우리는 모든 것을 알고 있습니다)<br>\n",
        "nous savons en ville (우리는 마을에서 알고)<br>\n",
        "<br>\n",
        "que vous etes grandes 당신이 키가 크다는 것을)<br>\n",
        "comme vous etes grands (당신이 얼마나 큰)<br>\n",
        "<br>\n",
        "je suis si epuise (너무 지쳤어)<br>\n",
        "je suis si idiot (나는 정말 바보 야)<br>"
      ],
      "metadata": {
        "id": "xY83oT4dNtsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 회고\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "s0qqis_FGXvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어렵고,<br>\n",
        "신기하고,<br>\n",
        "경이롭다.<br>\n",
        "<br>\n",
        "언어 쪽은 역시 나랑 안 맞는다는 것을 다시 느끼게 해주었으며,<br>\n",
        "<br>\n",
        "우선 보는 것을 먼저 습득하게 되면,<br>\n",
        "언어적인 것으로 가야 하고,<br>\n",
        "궁극적으로는 인간이 감각이나 사고를 뛰어넘는 영역으로 가야 하지 않을까 생각해 보았습니다."
      ],
      "metadata": {
        "id": "6edhMi8nOxel"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsFO8mJuTVWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}