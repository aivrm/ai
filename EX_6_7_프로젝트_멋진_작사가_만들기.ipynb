{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX 6-7. 프로젝트: 멋진 작사가 만들기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1mPqJB5eRBgmTANHaOGjgOxp3rI93W43Q",
      "authorship_tag": "ABX9TyMeRQB8P0haXbdiUBbfw7mP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivrm/ai/blob/main/EX_6_7_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%EB%A9%8B%EC%A7%84_%EC%9E%91%EC%82%AC%EA%B0%80_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6-7. 프로젝트: 멋진 작사가 만들기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UCcO_u3Rpb3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2XqXHSZpedd",
        "outputId": "09fa8039-ccbd-40ca-e351-80cf84e16fb0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 1. 데이터 다운로드\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SRZBFXtNGpFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 2. 데이터 읽어오기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1CFThaE7GvZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GxMeKIvWpLqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d5c240-8bdf-474b-cc7b-6efa1a10cd96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os, re\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/_aiffel'\n",
        "# txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
        "txt_file_path = BASE_DIR+'/aiffel/lyricist/data/lyrics/*'\n",
        "txt_files_path_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_files_path_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 3. 데이터 정제\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Nl0zCjFsG0U7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
        "\n",
        "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
        "\n",
        "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
        "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
      ],
      "metadata": {
        "id": "K8KE7FT6G7ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "        \n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "YF_w71Vr9Qoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e08c9a1-c06d-4566-b7bd-3ce8eeaf28ef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuRX3EmEv472",
        "outputId": "ac806acd-d2a3-4886-bc87-382533ea46d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if len(sentence) > 15: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKpF2dOxujH0",
        "outputId": "b54b9f5a-6a54-4ed3-fa65-f4f50d3359f4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> baby , baby <end>',\n",
              " '<start> ooh , ooh <end>',\n",
              " '<start> baby , baby <end>',\n",
              " '<start> ooh , ooh <end>',\n",
              " '<start> baby , baby <end>',\n",
              " '<start> i miss my baby <end>',\n",
              " '<start> i miss my baby <end>',\n",
              " '<start> you played it <end>',\n",
              " '<start> you played it <end>',\n",
              " '<start> oh , anymore <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 4. 평가 데이터셋 분리\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uNftFnL0H4-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize() 함수로 데이터를 Tensor로 변환한 후,<br>\n",
        "sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다.<br>\n",
        "단어장의 크기는 12,000 이상 으로 설정하세요!<br>\n",
        "총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
      ],
      "metadata": {
        "id": "j71Tv1P7DeKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
        "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "def tokenize(corpus):\n",
        "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    print(tokenizer)\n",
        "    print(type(tokenizer))\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tuF8CHRx58T",
        "outputId": "93fef97d-1393-41f7-a0a4-cf98c4a08379"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras_preprocessing.text.Tokenizer object at 0x7f3526041dd0>\n",
            "<class 'keras_preprocessing.text.Tokenizer'>\n",
            "[[   2   24    5 ...    0    0    0]\n",
            " [   2   44    5 ...    0    0    0]\n",
            " [   2   24    5 ...    0    0    0]\n",
            " ...\n",
            " [   2 1074 1075 ...    0    0    0]\n",
            " [   2 1074 1075 ...    0    0    0]\n",
            " [   2 1074 1075 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f3526041dd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfg8HhOSyWRM",
        "outputId": "84e3a4c4-07ea-415f-ba44-bc9fff536bb6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2 24  5 24  3  0  0  0  0  0]\n",
            " [ 2 44  5 44  3  0  0  0  0  0]\n",
            " [ 2 24  5 24  3  0  0  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeuhjT5LyYYA",
        "outputId": "08516f98-63e9-48fc-bda2-73bac0756e61"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : .\n",
            "7 : you\n",
            "8 : oh\n",
            "9 : it\n",
            "10 : me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsziazx6yg3w",
        "outputId": "5e382540-c1df-450a-9528-47a2a4bf38ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2 24  5 24  3  0  0  0  0  0  0  0]\n",
            "[24  5 24  3  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <코드 작성>\n",
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=34)"
      ],
      "metadata": {
        "id": "yjPENqTMrcd4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train.shape, enc_val.shape, dec_train.shape, dec_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkt8-PcKEh9c",
        "outputId": "34e4c4e6-b3f9-4508-9137-ddf9fc1ff279"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11296, 12), (2824, 12), (11296, 12), (2824, 12))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "ds_train = ds_train.shuffle(BUFFER_SIZE)\n",
        "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "ds_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRKT6kX1yvFZ",
        "outputId": "8638c11b-928c-4221-deae-8dc2a8da3720"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 12), dtype=tf.int32, name=None), TensorSpec(shape=(256, 12), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "ds_val = ds_val.shuffle(BUFFER_SIZE)\n",
        "ds_val = ds_val.batch(BATCH_SIZE, drop_remainder=True)\n",
        "ds_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moRY0kzJXqYa",
        "outputId": "bd10af52-3406-42d9-e7a4-5e6ed759b877"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 12), dtype=tf.int32, name=None), TensorSpec(shape=(256, 12), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Step 5. 인공지능 만들기\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KxM2Ju1-HZlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "SlKfFL3Sy2gn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_size = 256\n",
        "# hidden_size = 1024\n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "PZdfsTr3LwEV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in ds_train.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "lyricist(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Kxaf-Vy6a0",
        "outputId": "f0c4a69a-733e-421e-f16d-38abf2607d69"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 12, 12001), dtype=float32, numpy=\n",
              "array([[[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [-1.38541567e-04, -5.59218788e-05,  3.35400546e-05, ...,\n",
              "         -1.32436951e-04,  2.09406615e-04,  2.24464631e-04],\n",
              "        [-1.50381122e-04, -1.90594626e-04,  8.48600976e-05, ...,\n",
              "         -1.57226663e-04,  4.37522016e-04,  4.09215456e-04],\n",
              "        ...,\n",
              "        [-1.17777367e-04, -7.92014820e-04,  9.82160680e-04, ...,\n",
              "          8.95825622e-04,  1.56365579e-03,  2.18114676e-03],\n",
              "        [-1.75750771e-04, -1.04601693e-03,  1.10965827e-03, ...,\n",
              "          1.19454728e-03,  1.62180117e-03,  2.29245424e-03],\n",
              "        [-2.42254304e-04, -1.33179291e-03,  1.23236212e-03, ...,\n",
              "          1.47638330e-03,  1.65655173e-03,  2.34865956e-03]],\n",
              "\n",
              "       [[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [ 8.48829877e-05,  4.30717264e-05, -1.38457153e-05, ...,\n",
              "         -1.99451621e-04,  4.00909130e-06,  3.11571639e-04],\n",
              "        [ 2.26999458e-04,  1.01014870e-04, -7.91062484e-06, ...,\n",
              "         -3.92890070e-04, -3.02766475e-05,  4.03806800e-04],\n",
              "        ...,\n",
              "        [ 4.31587796e-05, -8.90321040e-04,  1.04223739e-03, ...,\n",
              "          8.36711028e-04,  1.64522999e-03,  2.42156349e-03],\n",
              "        [-4.82468968e-05, -1.19462062e-03,  1.17877440e-03, ...,\n",
              "          1.14008063e-03,  1.71227055e-03,  2.46721762e-03],\n",
              "        [-1.42846766e-04, -1.51601306e-03,  1.30681542e-03, ...,\n",
              "          1.42614904e-03,  1.75301335e-03,  2.46513495e-03]],\n",
              "\n",
              "       [[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [-1.32972666e-04,  1.28274478e-04,  3.25091620e-04, ...,\n",
              "          7.80524497e-05,  7.43259079e-05,  2.91636330e-04],\n",
              "        [-1.04936473e-04,  3.38323105e-07,  7.81859504e-04, ...,\n",
              "          3.74668103e-04, -5.61283050e-05,  2.15304273e-04],\n",
              "        ...,\n",
              "        [ 4.22721205e-04, -5.37980522e-04,  1.81790465e-03, ...,\n",
              "          5.45130868e-04,  1.70358934e-03,  1.69775530e-03],\n",
              "        [ 3.26096604e-04, -7.56868743e-04,  1.80349289e-03, ...,\n",
              "          8.12383892e-04,  1.83455972e-03,  1.92255364e-03],\n",
              "        [ 2.18667643e-04, -1.01552904e-03,  1.79478480e-03, ...,\n",
              "          1.08922331e-03,  1.91223039e-03,  2.07842700e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [-6.07619004e-05,  1.68090221e-04, -6.12035801e-05, ...,\n",
              "         -1.90739447e-04,  1.45580954e-04, -4.65032790e-05],\n",
              "        [-6.68653520e-05,  3.53589567e-04, -3.02755216e-04, ...,\n",
              "         -2.48365803e-04,  5.71847413e-05, -1.64297322e-04],\n",
              "        ...,\n",
              "        [-3.31158604e-04, -4.31007829e-05,  7.19688658e-04, ...,\n",
              "          6.84943981e-04,  1.13335147e-03,  1.74073060e-03],\n",
              "        [-3.98990989e-04, -3.82015831e-04,  9.07947833e-04, ...,\n",
              "          9.93529567e-04,  1.23196363e-03,  1.92760141e-03],\n",
              "        [-4.67528211e-04, -7.50739477e-04,  1.08178088e-03, ...,\n",
              "          1.28928979e-03,  1.30730716e-03,  2.04845378e-03]],\n",
              "\n",
              "       [[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [-1.88108868e-04,  3.75462550e-05,  7.82495317e-06, ...,\n",
              "         -5.19232475e-04,  2.11118997e-04,  2.82249675e-04],\n",
              "        [-1.18461554e-04,  9.47228764e-05,  4.24536265e-05, ...,\n",
              "         -8.34520964e-04,  2.34743755e-04,  3.05824971e-04],\n",
              "        ...,\n",
              "        [ 8.58515250e-06, -9.41082195e-04,  1.21855852e-03, ...,\n",
              "          6.76444150e-04,  1.57254469e-03,  2.16276338e-03],\n",
              "        [-6.20033461e-05, -1.24971871e-03,  1.35096163e-03, ...,\n",
              "          1.00821513e-03,  1.61855563e-03,  2.23335088e-03],\n",
              "        [-1.41982076e-04, -1.57346879e-03,  1.47092273e-03, ...,\n",
              "          1.31559616e-03,  1.64686400e-03,  2.25881324e-03]],\n",
              "\n",
              "       [[-4.61391282e-05,  6.27216214e-05,  9.78134267e-05, ...,\n",
              "         -1.37110677e-04,  1.01833626e-04,  1.20703808e-04],\n",
              "        [-7.06881183e-05, -5.10858517e-05,  1.41394121e-04, ...,\n",
              "         -2.19887443e-04, -1.36323739e-04,  1.70305240e-04],\n",
              "        [-1.16497467e-04, -1.66876052e-04,  3.08522838e-04, ...,\n",
              "         -2.69960437e-04, -4.39384348e-05,  3.32757510e-04],\n",
              "        ...,\n",
              "        [-1.34963731e-04, -5.23816852e-04,  1.37831247e-03, ...,\n",
              "          8.06914468e-04,  1.45134679e-03,  2.34544487e-03],\n",
              "        [-1.64423574e-04, -7.66767655e-04,  1.47119700e-03, ...,\n",
              "          1.10991963e-03,  1.55635725e-03,  2.46793684e-03],\n",
              "        [-2.05053715e-04, -1.04892452e-03,  1.55561802e-03, ...,\n",
              "          1.39679213e-03,  1.62794022e-03,  2.52559129e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyricist.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVHYK5Iyy_uO",
        "outputId": "0f54dc99-bc6a-4793-f207-0b1bc8129fe8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  3072256   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer와 loss등은 차차 배웁니다 \n",
        "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "A7GhAYQlNhEu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss는 아래 제시된 Loss 함수를 그대로 사용하세요!"
      ],
      "metadata": {
        "id": "zDOrbWMXPeHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "e6Ie4V0NNhqD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile\n",
        "lyricist.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "# model.fit() 함수에 들어가는 다양한 인자를 알고 싶다면 아래의 문서를 참고하세요. \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "\n",
        "#lyricist.fit(dataset, epochs=10)\n",
        "lyricist.fit(ds_train,\n",
        "             batch_size=256,\n",
        "             epochs=10,\n",
        "             validation_data=ds_val,\n",
        "             verbose=1)"
      ],
      "metadata": {
        "id": "ifszNq_V-Go1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e334f93-8b98-469b-b81c-c7a4f143c4a3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 3.0142 - val_loss: 1.9794\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.7371 - val_loss: 1.6485\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.4881 - val_loss: 1.5367\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 4s 88ms/step - loss: 1.4130 - val_loss: 1.4906\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.3769 - val_loss: 1.4747\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.3554 - val_loss: 1.4623\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.3368 - val_loss: 1.4494\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.3169 - val_loss: 1.4362\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.2937 - val_loss: 1.4203\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 4s 86ms/step - loss: 1.2679 - val_loss: 1.4004\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f35a4faa9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "jqcYtUOWzIPy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "ujSBn60ErvG7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate_text(model, tokenizer, init_sentence=\"<start> he\")\n",
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "_whMWSSWzM4W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d236054d-8a96-4fcf-d913-443ee3563c39"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love it <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i miss\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vl6bJ5FihyRd",
        "outputId": "718bcabd-4558-4e6d-fa32-92351b53358b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i miss it <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 회고\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "K-KtvC8OfHD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 노드는, 초반에는 재미있었는데, 후반에서는 개인적으로 힘들고 혼란스러웠습니다.<br>\n",
        "RNN을 처음으로 접한 기회였지만, 기대했던 결과가 나오지 않아서 아쉽고,<br>\n",
        "무엇이 문제였는지 좀 더 공부를 하여야 하겠습니다.\n"
      ],
      "metadata": {
        "id": "sCRglgQBillE"
      }
    }
  ]
}